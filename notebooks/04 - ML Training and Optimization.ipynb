{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Training and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Full and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Hyperparameter Optimization and Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import learning_curve, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "ML_columns = [column for column in loans_data.columns if loans_data[column].dtype == int or loans_data[column].dtype == float or loans_data[column].dtype == bool]\n",
    "\n",
    "sample_data = loans_data.sample(10000)\n",
    "# Defining inputs and outputs of dataset\n",
    "X = sample_data[ML_columns].drop('loan_status_num', axis='columns').values\n",
    "y = np.vstack(sample_data[ML_columns]['loan_status_num'].values)\n",
    "# Splitting between trainnig and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of training inputs: {X_train.shape}\")\n",
    "print(f\"Shape of testing inputs: {X_test.shape}\")\n",
    "print(f\"Shape of training outputs: {y_train.shape}\")\n",
    "print(f\"Shape of testing outputs: {y_test.shape}\")\n",
    "\n",
    "# Scaling the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "scaler_X.fit(X)\n",
    "X_train = scaler_X.transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y.fit(y)\n",
    "y_train = scaler_y.transform(y_train)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "logistic_model = LogisticRegression(class_weight='balanced', max_iter=1000, C=0.01)\n",
    "\n",
    "# Train the model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    logistic_model, X_train, y_train, cv=5, n_jobs=-1, \n",
    "    train_sizes=np.linspace(.1, 1.0, 5))\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "coefficients = np.abs(logistic_model.coef_[0])\n",
    "\n",
    "# Combining feature names and their corresponding coefficients\n",
    "features, coefs = zip(*sorted(zip(ML_columns, coefficients), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "features = np.array(features)\n",
    "coefs = np.array(coefs)\n",
    "\n",
    "features = features[coefs > 0]\n",
    "coefs = coefs[coefs > 0]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize = [30*1.62, 15])\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizing the confusion matrix\n",
    "\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('Confusion Matrix')\n",
    "ax[0].set_ylabel('Actual Labels')\n",
    "ax[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "ax[1].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "ax[1].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "ax[1].set_title('Learning Curve')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Score')\n",
    "ax[1].legend(loc=\"best\")\n",
    "\n",
    "sns.barplot(x=coefs, y=features, edgecolor=\"black\", ax=ax[2])\n",
    "ax[2].set_title(\"Correlation between Loan status and Numeric Features\")\n",
    "ax[2].set_xlabel('Correlation')\n",
    "ax[2].set_ylabel('Numerical Features')\n",
    "ax[2].tick_params(axis='y', labelsize=5)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # Since we're setting this at the sys level, it should not be overridden\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # Also affect subprocesses\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "print(\"Starting Hyperparameter optimization run for Logistic Model\")\n",
    "\n",
    "n_reps = 100\n",
    "max_iter = 100000\n",
    "\n",
    "print(f\"Number of repetitions: {n_reps}\")\n",
    "\n",
    "param_dist = {\n",
    "    'C': [10**i for i in np.linspace(-4, 4, num=100)],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet', None],\n",
    "    'l1_ratio': np.linspace(0, 1, num=100),\n",
    "}\n",
    "\n",
    "logistic_model = LogisticRegression(class_weight='balanced', max_iter=max_iter)\n",
    "\n",
    "random_search = RandomizedSearchCV(logistic_model, param_distributions=param_dist, n_iter=n_reps, cv=5, random_state=42, verbose=10, error_score=np.nan)\n",
    "\n",
    "print(\"Fitting Random Search\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "param_combination_details = []\n",
    "\n",
    "for i, (params, mean_test_score) in enumerate(zip(random_search.cv_results_['params'], random_search.cv_results_['mean_test_score']), 1):\n",
    "    try:\n",
    "        print(f\"Model ID: {i}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "\n",
    "        current_model = random_search.best_estimator_.set_params(**params)\n",
    "\n",
    "        # Calculate learning curve data\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            current_model, X_train, y_train, cv=5, n_jobs=-1, \n",
    "            train_sizes=np.linspace(.1, 1.0, 5))\n",
    "\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "        current_model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = current_model.predict(X_test)\n",
    "\n",
    "        coefficients = np.abs(current_model.coef_[0])\n",
    "\n",
    "        features, coefs = zip(*sorted(zip(ML_columns, coefficients), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        features = np.array(features)\n",
    "        coefs = np.array(coefs)\n",
    "\n",
    "        features = features[coefs > 0]\n",
    "        coefs = coefs[coefs > 0]\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizing the confusion matrix\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize = [30*1.62, 15])\n",
    "\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=ax[0])\n",
    "        ax[0].set_title('Confusion Matrix')\n",
    "        ax[0].set_ylabel('Actual Labels')\n",
    "        ax[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "        ax[1].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        ax[1].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        ax[1].set_title('Learning Curve')\n",
    "        ax[1].set_xlabel('Training Examples')\n",
    "        ax[1].set_ylabel('Score')\n",
    "        ax[1].legend(loc=\"best\")\n",
    "\n",
    "        sns.barplot(x=coefs, y=features, edgecolor=\"black\", ax=ax[2])\n",
    "        ax[2].set_title(\"Correlation between Loan status and Numeric Features\")\n",
    "        ax[2].set_xlabel('Correlation')\n",
    "        ax[2].set_ylabel('Numerical Features')\n",
    "\n",
    "        fig_path = f\"../figures/LogisticRegression_{str(i).zfill(len(str(n_reps)))}.png\"\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(fig_path)\n",
    "        plt.close(fig)\n",
    "        param_combination_details.append({\n",
    "            'id': i,\n",
    "            'params': params,\n",
    "            'accuracy': mean_test_score,\n",
    "            'figure_path': fig_path\n",
    "        })\n",
    "        print(f\"Accuracy:{mean_test_score}\\n------------------------\")\n",
    "    except:\n",
    "        print(\"Likely Bad Combination?\")\n",
    "param_combination_df = pd.DataFrame(param_combination_details)\n",
    "param_combination_df.to_csv('../data/LogisticRegression_RandomizedSearchCV.csv', index=False)\n",
    "print(f'FINAL RESULTS')\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best score: {random_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "print(\"Starting Hyperparameter optimization run for Logistic Model\")\n",
    "\n",
    "n_reps = 1000\n",
    "max_iter = 100000\n",
    "\n",
    "print(f\"Number of repetitions: {n_reps}\")\n",
    "\n",
    "# Define the parameter space for BayesSearchCV\n",
    "# search_space = {\n",
    "#     'C': Real(1e-5, 1e5, 'log-uniform'),\n",
    "#     'solver': Categorical(['liblinear']),\n",
    "#     'penalty': Categorical(['l2', 'l1']),\n",
    "#     'l1_ratio': Real(0, 1)\n",
    "# }\n",
    "# best ~ 0.65\n",
    "\n",
    "search_space = {\n",
    "    'C': Real(1e-5, 1e5, 'log-uniform'),\n",
    "    'solver': Categorical(['newton-cg']),\n",
    "    'penalty': Categorical(['l2', None]),\n",
    "    'l1_ratio': Real(0, 1)\n",
    "}\n",
    "\n",
    "def progress_reporter(optim_result):\n",
    "    # This function can be customized to print or log the information you're interested in.\n",
    "    # For instance, you can print the current iteration number:\n",
    "    iteration = len(optim_result.x_iters)\n",
    "    print(f\"Iteration {iteration}/{n_reps}, Current best score: {-optim_result.fun}\")\n",
    "\n",
    "logistic_model = LogisticRegression(class_weight='balanced', max_iter=max_iter)\n",
    "\n",
    "# Define BayesSearchCV\n",
    "bayes_search = BayesSearchCV(logistic_model, search_space, n_iter=n_reps, cv=5, random_state=42, verbose=10, n_jobs=-1)\n",
    "\n",
    "bayes_search.fit(X_train, y_train, callback=progress_reporter)\n",
    "\n",
    "# Processing results\n",
    "param_combination_details = []\n",
    "for i, params in enumerate(bayes_search.cv_results_['params'], 1):\n",
    "    print(f\"Model ID: {i}\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "\n",
    "    current_model = bayes_search.best_estimator_.set_params(**params)\n",
    "\n",
    "    # Calculate learning curve data\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        current_model, X_train, y_train, cv=5, n_jobs=-1, \n",
    "        train_sizes=np.linspace(.1, 1.0, 5))\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    current_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = current_model.predict(X_test)\n",
    "\n",
    "    coefficients = np.abs(current_model.coef_[0])\n",
    "\n",
    "    features, coefs = zip(*sorted(zip(ML_columns, coefficients), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    features = np.array(features)\n",
    "    coefs = np.array(coefs)\n",
    "\n",
    "    features = features[coefs > 0]\n",
    "    coefs = coefs[coefs > 0]\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizing the confusion matrix\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize = [30*1.62, 15])\n",
    "\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=ax[0])\n",
    "    ax[0].set_title('Confusion Matrix')\n",
    "    ax[0].set_ylabel('Actual Labels')\n",
    "    ax[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "    ax[1].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    ax[1].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    ax[1].set_title('Learning Curve')\n",
    "    ax[1].set_xlabel('Training Examples')\n",
    "    ax[1].set_ylabel('Score')\n",
    "    ax[1].legend(loc=\"best\")\n",
    "\n",
    "    sns.barplot(x=coefs, y=features, edgecolor=\"black\", ax=ax[2])\n",
    "    ax[2].set_title(\"Correlation between Loan status and Numeric Features\")\n",
    "    ax[2].set_xlabel('Correlation')\n",
    "    ax[2].set_ylabel('Numerical Features')\n",
    "\n",
    "    fig_path = f\"../figures/LogisticRegression_BayesSearchCV_{str(i).zfill(len(str(n_reps)))}.png\"\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fig_path)\n",
    "    plt.close(fig)\n",
    "    param_combination_details.append({\n",
    "        'id': i,\n",
    "        'params': params,\n",
    "        'accuracy': np.mean(test_scores_mean),\n",
    "        'figure_path': fig_path\n",
    "    })\n",
    "    print(f\"Accuracy:{np.mean(test_scores_mean)}\\n------------------------\")\n",
    "param_combination_df = pd.DataFrame(param_combination_details)\n",
    "param_combination_df.to_csv('../data/LogisticRegression_BayesSearchCV.csv', index=False)\n",
    "print(f'FINAL RESULTS')\n",
    "print(f\"Best parameters: {bayes_search.best_params_}\")\n",
    "print(f\"Best score: {bayes_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import sys\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "matplotlib.use('Agg')\n",
    "print(\"Starting Hyperparameter optimization run for Logistic Model\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # Since we're setting this at the sys level, it should not be overridden\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # Also affect subprocesses\n",
    "    \n",
    "n_reps = 100\n",
    "max_iter = 100000\n",
    "\n",
    "print(f\"Number of repetitions: {n_reps}\")\n",
    "\n",
    "# Define the parameter space for BayesSearchCV\n",
    "search_space = {\n",
    "    'C': Real(1e-4, 1e4, 'log-uniform'),\n",
    "    'solver': Categorical(['liblinear', 'saga']),\n",
    "    'penalty': Categorical(['l2', 'l1', 'elasticnet']),\n",
    "    'l1_ratio': Real(0, 1)\n",
    "}\n",
    "\n",
    "logistic_model = LogisticRegression(class_weight='balanced', max_iter=max_iter)\n",
    "\n",
    "class CustomLogisticRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, penalty='l2', max_iter=100, solver='lbfgs', l1_ratio=None):\n",
    "        # Explicitly list parameters\n",
    "        self.C = C\n",
    "        self.penalty = penalty\n",
    "        self.max_iter = max_iter\n",
    "        self.solver = solver\n",
    "        self.l1_ratio = l1_ratio\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Use the parameters in LogisticRegression\n",
    "        self.model = LogisticRegression(\n",
    "            C=self.C, \n",
    "            penalty=self.penalty, \n",
    "            max_iter=self.max_iter, \n",
    "            solver=self.solver, \n",
    "            l1_ratio=self.l1_ratio\n",
    "        )\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        params = self.model.get_params()\n",
    "\n",
    "        # Define invalid combinations\n",
    "        if params['solver'] == 'liblinear' and params['penalty'] == 'elasticnet':\n",
    "            return -np.inf  # Large negative value for invalid combination\n",
    "        if params['solver'] == 'liblinear' and params['penalty'] is None:\n",
    "            return -np.inf\n",
    "        if params['penalty'] == 'elasticnet' and params['solver'] != 'saga':\n",
    "            return np.inf\n",
    "\n",
    "        # Compute the score only if the combination is valid\n",
    "        return accuracy_score(y, y_pred)\n",
    "\n",
    "logistic_model_instance = CustomLogisticRegression()\n",
    "\n",
    "# BayesSearchCV with custom scorer\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=logistic_model_instance, \n",
    "    search_spaces=search_space, \n",
    "    n_iter=n_reps, \n",
    "    cv=5, \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "np.int = int\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Processing results\n",
    "param_combination_details = []\n",
    "for i, params in enumerate(bayes_search.cv_results_['params'], 1):\n",
    "    try:\n",
    "        print(f\"Model ID: {i}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "\n",
    "        current_model = bayes_search.best_estimator_.set_params(**params)\n",
    "\n",
    "        # Calculate learning curve data\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            current_model, X_train, y_train, cv=5, n_jobs=-1, \n",
    "            train_sizes=np.linspace(.1, 1.0, 5))\n",
    "\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "        current_model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = current_model.predict(X_test)\n",
    "\n",
    "        coefficients = np.abs(current_model.coef_[0])\n",
    "\n",
    "        features, coefs = zip(*sorted(zip(ML_columns, coefficients), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        features = np.array(features)\n",
    "        coefs = np.array(coefs)\n",
    "\n",
    "        features = features[coefs > 0]\n",
    "        coefs = coefs[coefs > 0]\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizing the confusion matrix\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize = [30*1.62, 15])\n",
    "\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=ax[0])\n",
    "        ax[0].set_title('Confusion Matrix')\n",
    "        ax[0].set_ylabel('Actual Labels')\n",
    "        ax[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "        ax[1].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        ax[1].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        ax[1].set_title('Learning Curve')\n",
    "        ax[1].set_xlabel('Training Examples')\n",
    "        ax[1].set_ylabel('Score')\n",
    "        ax[1].legend(loc=\"best\")\n",
    "\n",
    "        sns.barplot(x=coefs, y=features, edgecolor=\"black\", ax=ax[2])\n",
    "        ax[2].set_title(\"Correlation between Loan status and Numeric Features\")\n",
    "        ax[2].set_xlabel('Correlation')\n",
    "        ax[2].set_ylabel('Numerical Features')\n",
    "\n",
    "        fig_path = f\"../figures/LogisticRegression_BayesSearchCV_{str(i).zfill(len(str(n_reps)))}.png\"\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(fig_path)\n",
    "        plt.close(fig)\n",
    "        param_combination_details.append({\n",
    "            'id': i,\n",
    "            'params': params,\n",
    "            'accuracy': mean_test_score,\n",
    "            'figure_path': fig_path\n",
    "        })\n",
    "        print(f\"Accuracy:{mean_test_score}\\n------------------------\")\n",
    "    except:\n",
    "        print(\"Likely Bad Combination?\")\n",
    "param_combination_df = pd.DataFrame(param_combination_details)\n",
    "param_combination_df.to_csv('../data/LogisticRegression_BayesSearchCV.csv', index=False)\n",
    "print(f'FINAL RESULTS')\n",
    "print(f\"Best parameters: {bayes_search.best_params_}\")\n",
    "print(f\"Best score: {bayes_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPyOpt\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "import numpy as np\n",
    "\n",
    "# Define the domain (parameter space)\n",
    "domain = [{'name': 'C', 'type': 'continuous', 'domain': (1e-4, 1e4), 'transformation': 'log'},\n",
    "          {'name': 'solver', 'type': 'categorical', 'domain': (0, 1)},\n",
    "          {'name': 'penalty', 'type': 'categorical', 'domain': (0, 1, 2, 3)},\n",
    "          {'name': 'l1_ratio', 'type': 'continuous', 'domain': (0, 1)}]\n",
    "\n",
    "# Mapping for categorical variables\n",
    "solver_mapping = {0: 'liblinear', 1: 'saga'}\n",
    "penalty_mapping = {0: 'l2', 1: 'l1', 2: 'elasticnet', 3: None}\n",
    "\n",
    "\n",
    "def model_score(model, X_train, X_test, y_train, y_test, cv=5):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "    logistic_model, X_train, y_train, cv=5, n_jobs=-1, \n",
    "    train_sizes=np.linspace(.1, 1.0, 5))\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    \n",
    "    return test_scores_mean\n",
    "\n",
    "def is_valid_combination(params):\n",
    "    if params['solver'] == 'liblinear' and (params['penalty'] == 'elasticnet' or params['l1_ratio'] is not None):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Objective function\n",
    "def objective_function(x):\n",
    "    params = {\n",
    "        'C': x[0][0],\n",
    "        'solver': solver_mapping[int(x[0][1])],\n",
    "        'penalty': penalty_mapping[int(x[0][2])],\n",
    "        'l1_ratio': x[0][3]\n",
    "    }\n",
    "    if not is_valid_combination(params):\n",
    "        return np.nan\n",
    "    model = LogisticRegression(class_weight='balanced', max_iter=max_iter, **params)\n",
    "    return -model_score(model, X_train, X_test, y_train, y_test)  # Assuming lower is better\n",
    "\n",
    "bo = BayesianOptimization(f=objective_function, domain=domain, model_type='GP', acquisition_type='EI', maximize=True)\n",
    "bo.run_optimization(max_iter=50)\n",
    "\n",
    "print(bo.x_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the RandomForest model\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    rf_model, X_train, y_train, cv=5, n_jobs=-1, \n",
    "    train_sizes=np.linspace(.1, 1.0, 5))\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Accuracy and Classification Report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature Importances\n",
    "feature_importances = abs(rf_model.feature_importances_)\n",
    "\n",
    "# Combining feature names and their corresponding importances\n",
    "features, importances = zip(*sorted(zip(ML_columns, feature_importances), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 3, figsize=[15*1.62, 5])\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('Confusion Matrix')\n",
    "ax[0].set_ylabel('Actual Labels')\n",
    "ax[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "# Learning Curve\n",
    "ax[1].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "ax[1].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "ax[1].set_title('Learning Curve')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Score')\n",
    "ax[1].legend(loc=\"best\")\n",
    "\n",
    "# Feature Importances\n",
    "sns.barplot(x=importances, y=features, edgecolor=\"black\", ax=ax[2])\n",
    "ax[2].set_title(\"Feature Importances\")\n",
    "ax[2].set_xlabel('Relative Importance')\n",
    "ax[2].set_ylabel('Features')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# Initialize the MLPClassifier model\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', \n",
    "                          max_iter=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "\n",
    "# Learning curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    mlp_model, X_train, y_train, cv=5, train_sizes=np.linspace(.1, 1.0, 5))\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Accuracy and Classification Report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 2, figsize=[10*1.62, 5])  # Reduced to 1x2 layout\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('Confusion Matrix')\n",
    "ax[0].set_ylabel('Actual Labels')\n",
    "ax[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "# Learning Curve\n",
    "ax[1].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "ax[1].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "ax[1].set_title('Learning Curve')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Score')\n",
    "ax[1].legend(loc=\"best\")\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LendingClub",
   "language": "python",
   "name": "lendingclub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
