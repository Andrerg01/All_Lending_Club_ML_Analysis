{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning CSV Into SQLITE\n",
    "This notebook lays down the method for turning the downloaded CSV into an sqlite file, which is superior\n",
    "\n",
    "The CSV file can be downloaded from [here](https://www.kaggle.com/datasets/wordsforthewise/lending-club/)\n",
    "\n",
    "This notebook simply serves as a prototype, and the actual data should be processed from the script `scripts/01 - csv_to_sqlite.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Up csv_to_sqlite.py\")\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "print(\"Defining Classes\")\n",
    "class Logger:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.log_dir = config['logging']['out_dir']\n",
    "        self.tag = config['base']['tag']\n",
    "        self.file_path = os.path.join('outputs', self.tag, self.log_dir, 'log.txt')\n",
    "        self.verbose = config['logging']['verbose']\n",
    "        \n",
    "    def log(self, message):\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        datetime_string = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_message = f\"{datetime_string}: {message}\"\n",
    "        if self.verbose:\n",
    "            print(log_message)\n",
    "        with open(self.file_path, \"a\") as f:\n",
    "            f.write(f'{log_message}\\n')\n",
    "\n",
    "print(\"Defining Functions\")\n",
    "\n",
    "def convert_to_unix_time(date_str):\n",
    "    \"\"\"\n",
    "    Convert a date string to Unix time.\n",
    "    :param date_str: A string representing a date in 'Mon-Year' format (e.g., 'Dec-2015').\n",
    "    :return: Unix time as an integer.\n",
    "    \"\"\"\n",
    "    return pd.to_datetime(date_str).value // 10**9\n",
    "\n",
    "def cast_proper_type(column):\n",
    "    # Check for boolean conversion (including special case for 'Y' and 'N')\n",
    "    if all(val in [0, 1, '0', '1', 'Y', 'N', True, False] for val in column):\n",
    "        return column.replace({'Y': True, 'N': False}).astype(bool)\n",
    "\n",
    "    # Check for integer conversion\n",
    "    if all(pd.to_numeric(column, errors='coerce').notnull()) and (pd.to_numeric(column).dropna() % 1 == 0).all():\n",
    "        return pd.to_numeric(column, downcast='integer')\n",
    "\n",
    "    # Check for float conversion\n",
    "    if all(pd.to_numeric(column, errors='coerce').notnull()):\n",
    "        return pd.to_numeric(column, downcast='float')\n",
    "\n",
    "    # Default to string conversion\n",
    "    return column.astype(str)\n",
    "\n",
    "def map_dtype_to_sqlite(col_type):\n",
    "    if col_type.startswith('int') or col_type == 'bool':\n",
    "        return 'INTEGER'\n",
    "    elif col_type.startswith('float'):\n",
    "        return 'REAL'\n",
    "    else:  # Default case, particularly for 'object' and other unhandled types\n",
    "        return 'TEXT'\n",
    "        \n",
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        return f\"Created directory: {directory}\"\n",
    "    else:\n",
    "        return f\"Directory already exists: {directory}\"\n",
    "\n",
    "print(\"Reading Config File\")\n",
    "\n",
    "config_file_path = '../config/config.yml'\n",
    "\n",
    "root_path = '..'\n",
    "\n",
    "print(f\"Reading Config File {config_file_path}\")\n",
    "with open(config_file_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "print(\"Defining Variables and Creating Directories\")\n",
    "\n",
    "csv_file = f\"{root_path}/{config['data']['input_csv']}\"\n",
    "sqlite_file = config['data']['output_sqlite']\n",
    "tag = config['base']['tag']\n",
    "full_column_names_file = f\"{root_path}/{config['data']['column_names_file']}\"\n",
    "column_descriptions_file = f\"{root_path}/{config['data']['column_descriptions_file']}\"\n",
    "\n",
    "sqlite_file = os.path.join(f'{root_path}/outputs/{tag}/data/{sqlite_file}')\n",
    "\n",
    "print(create_directory_if_not_exists(os.path.join(f'{root_path}/outputs/{tag}/data/')))\n",
    "print(create_directory_if_not_exists(os.path.join(f'{root_path}/outputs/{tag}/log/')))\n",
    "print(create_directory_if_not_exists(os.path.join(f'{root_path}/outputs/{tag}/figure/')))\n",
    "print(create_directory_if_not_exists(os.path.join(f'{root_path}/outputs/{tag}/stats/')))\n",
    "print(create_directory_if_not_exists(os.path.join(f'{root_path}/outputs/{tag}/reports/')))\n",
    "\n",
    "print(\"Done with initial setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading Columns Full Names\")\n",
    "\n",
    "with open(full_column_names_file, 'r') as file:\n",
    "    column_full_names = json.load(file)\n",
    "    \n",
    "print(\"Reading Columns Descriptions\")\n",
    "\n",
    "with open(column_descriptions_file, 'r') as file:\n",
    "    column_descriptions = json.load(file)\n",
    "\n",
    "print('Loading data CSV')\n",
    "df = pd.read_csv(csv_file, low_memory=False).replace('nan',None)\n",
    "\n",
    "print('Loading Metadata')\n",
    "metadata = df.iloc[-2:]['id'].values\n",
    "df = df.iloc[:-2]\n",
    "\n",
    "print(\"Done with Loading Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Date Columns to UNIX Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finding Date Columns')\n",
    "\n",
    "dates_columns = []\n",
    "for column in sorted(df.columns):\n",
    "    samples = df[column].dropna().unique()\n",
    "    if len(samples) > 0 and len(str(samples[0])) == 8 and str(samples[0])[3] == '-':\n",
    "        dates_columns += [column]\n",
    "\n",
    "print('Creating UNIX Timestamp Columns for Dates')\n",
    "for column in dates_columns:\n",
    "    print(f\"Converting column {column} to UNIX time\")\n",
    "\n",
    "    df[f'{column}_dt'] = pd.to_datetime(df[column], format='%b-%Y', errors='coerce')\n",
    "\n",
    "    df[f'{column}_dt'].fillna(pd.Timestamp('1970-01-01'), inplace=True)\n",
    "\n",
    "    df[f'{column}_unix'] = (df[f'{column}_dt'] - pd.Timestamp('1970-01-01')).dt.total_seconds().astype(int)\n",
    "    \n",
    "print('Done Converting Date Columns to UNIX Timestamps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating interesting columns')\n",
    "\n",
    "df['term_months'] = df['term'].astype('str').apply(lambda x: int(x[1:3]) if x != 'nan' else -1)\n",
    "\n",
    "df['emp_length_years'] = df['emp_length'].astype('str').apply(lambda x: int(x.split(' ')[0].replace('+','')) if x != '< 1 year' and x != 'nan' else -1)\n",
    "\n",
    "df['id'] = range(len(df))\n",
    "\n",
    "print(\"Dropping uninteresting columns\")\n",
    "\n",
    "drop_columns = ['member_id', 'term', 'emp_length'] + \\\n",
    "                dates_columns + \\\n",
    "                [f'{col}_dt' for col in dates_columns]\n",
    "\n",
    "for column in drop_columns:\n",
    "    print(f'Dropping column {column}')\n",
    "    if column in df.columns:\n",
    "        df.drop(column, axis='columns', inplace=True)\n",
    "\n",
    "print(\"Filling NaNs on some columns\")\n",
    "\n",
    "fillna_neg_one_columns = ['tot_coll_amt', 'tot_cur_bal', 'all_util', 'annual_inc_joint', 'bc_open_to_buy',\n",
    "                         'deferral_term', 'collection_recovery_fee', 'hardship_last_payment_amount',\n",
    "                         'hardship_payoff_balance_amount', 'max_bal_bc', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op',\n",
    "                         'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mths_since_last_delinq', 'mths_since_last_major_derog',\n",
    "                         'mths_since_last_record', 'mths_since_rcnt_il', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n",
    "                         'mths_since_recent_inq', 'mths_since_recent_revol_delinq', 'revol_bal_joint',\n",
    "                         'sec_app_fico_range_high', 'sec_app_fico_range_low', 'sec_app_mort_acc', \n",
    "                         'sec_app_mths_since_last_major_derog', 'settlement_amount', 'settlement_percentage',\n",
    "                         'settlement_term', 'zip_code', 'total_rev_hi_lim', 'tot_hi_cred_lim', 'total_bc_limit',\n",
    "                         'total_il_high_credit_limit']\n",
    "\n",
    "for column in fillna_neg_one_columns:\n",
    "    print(f'Filling NaNs in column {column} with -1')\n",
    "    df[column].fillna(-1, inplace=True)\n",
    "    \n",
    "fillna_zero_columns = ['acc_now_delinq', 'acc_open_past_24mths', 'annual_inc', 'avg_cur_bal',\n",
    "                       'chargeoff_within_12_mths', 'collections_12_mths_ex_med', 'delinq_2yrs',\n",
    "                       'delinq_amnt', 'hardship_amount', 'hardship_dpd', 'hardship_length',\n",
    "                       'inq_fi', 'inq_last_12m', 'inq_last_6mths', 'mort_acc', 'num_accts_ever_120_pd',\n",
    "                       'num_actv_bc_tl', 'num_actv_rev_tl', 'num_tl_120dpd_2m', 'open_acc_6m',\n",
    "                       'open_act_il', 'open_il_12m', 'open_il_24m', 'open_rv_12m', 'open_rv_24m',\n",
    "                       'orig_projected_additional_accrued_interest', 'sec_app_chargeoff_within_12_mths',\n",
    "                       'sec_app_collections_12_mths_ex_med', 'sec_app_inq_last_6mths', 'sec_app_num_rev_accts',\n",
    "                       'sec_app_open_acc', 'sec_app_open_act_il', 'total_bal_il', 'total_cu_tl', 'open_acc',\n",
    "                       'pub_rec', 'total_acc', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl',\n",
    "                       'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_30dpd', 'num_tl_90g_dpd_24m',\n",
    "                       'num_tl_op_past_12m', 'pub_rec_bankruptcies', 'tax_liens', 'total_bal_ex_mort']\n",
    "\n",
    "for column in fillna_zero_columns:\n",
    "    print(f'Filling NaNs in column {column} with 0')\n",
    "    df[column].fillna(0, inplace=True)\n",
    "    \n",
    "fillna_one_columns = ['bc_util', 'dti', 'dti_joint', 'il_util', 'revol_util', 'sec_app_revol_util']\n",
    "for column in fillna_one_columns:\n",
    "    print(f'Filling NaNs in column {column} with 1')\n",
    "    df[column].fillna(1, inplace=True)\n",
    "    \n",
    "fillna_N_columns = ['debt_settlement_flag', 'hardship_flag']\n",
    "for column in fillna_N_columns:\n",
    "    print(f'Filling NaNs in column {column} with \\'N\\'')\n",
    "    df[column].fillna('N', inplace=True)\n",
    "    \n",
    "fillna_empty_string_columns = ['desc', 'emp_title', 'addr_state', 'application_type', 'disbursement_method',\n",
    "                              'hardship_loan_status', 'hardship_reason', 'hardship_status', 'settlement_status',\n",
    "                              'title', 'verification_status_joint', 'hardship_type']\n",
    "for column in fillna_empty_string_columns:\n",
    "    print(f'Filling NaNs in column {column} with an empty string')\n",
    "    df[column].fillna('', inplace=True)\n",
    "    \n",
    "fillna_100_columns = ['pct_tl_nvr_dlq', 'percent_bc_gt_75']\n",
    "\n",
    "for column in fillna_100_columns:\n",
    "    print(f'Filling NaNs in column {column} with 100')\n",
    "    df[column].fillna(100, inplace=True)\n",
    "    \n",
    "\n",
    "dropna_columns = ['fico_range_high', 'fico_range_low', 'funded_amnt', 'funded_amnt_inv', 'grade']\n",
    "print(f'Dropping columns: {dropna_columns}')\n",
    "df.dropna(subset=dropna_columns, axis=0, inplace=True)    \n",
    "\n",
    "print(\"Modifying Columns\")\n",
    "\n",
    "df['emp_title'] = df['emp_title'].astype('str').apply(lambda x: x.lower().strip().replace(',','-').replace('  ',' '))\n",
    "\n",
    "df['zip_code'] = df['zip_code'].astype('str').apply(lambda x: x[:3] if x != '' else x)\n",
    "\n",
    "df['zip_code'] = df['zip_code'].str.lstrip('0')\n",
    "\n",
    "print(\"Done with Creating and Dropping Columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting columns to proper types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in sorted(df.columns):\n",
    "    print(f'Converting elements of column \\'{column}\\' from \\'{str(df[column].dtype)}\\' to \\'{str(cast_proper_type(df[column]).dtype)}\\'')\n",
    "          \n",
    "    df[column] = cast_proper_type(df[column])\n",
    "    \n",
    "print(\"Done Casting columns to proper types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Metadata Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Metadata Dataframe\")\n",
    "\n",
    "metadata = pd.DataFrame({s.split(': ')[0]:[int(s.split(': ')[-1])] for s in metadata})\n",
    "\n",
    "print(\"Done Creating Metadata Dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Descriptions DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_descriptions = {\n",
    "    'loans_data':f'Data for all the loans available in the database, downloaded as a csv, cleaned, and put into a sqlite file on 2023-12-06. Original download from https://www.kaggle.com/code/pavlofesenko/minimizing-risks-for-loan-investments. Contains columns: {\", \".join(df.columns)}.',\n",
    "    'metadata':f'Metadata provided on the downloaded data, providing the total amount funded in different policy codes. Contains columns: {\", \".join(metadata.columns)}.',\n",
    "    'descriptions':f'A table containing a written description of each available table, and column. Contains columns: loans_data, metadata, descriptions, {\", \".join(df.columns)}, {\", \".join(metadata.columns)}.'\n",
    "}\n",
    "\n",
    "metadata_columns_description = column_descriptions['metadata']\n",
    "\n",
    "loans_data_columns_description = column_descriptions['loans_data']\n",
    "\n",
    "descriptions_columns_description = column_descriptions['descriptions']\n",
    "\n",
    "descriptions = {'name':[], 'full_name':[], 'type':[], 'location':[], 'description':[], 'data_type':[]}\n",
    "    \n",
    "for key, value in table_descriptions.items():\n",
    "    descriptions['name'] += [key]\n",
    "    descriptions['full_name'] += [key]\n",
    "    descriptions['type'] += ['table']\n",
    "    descriptions['location'] += ['root']\n",
    "    descriptions['description'] += [value]\n",
    "    descriptions['data_type'] += ['TABLE']\n",
    "    \n",
    "for key, value in metadata_columns_description.items():\n",
    "    descriptions['name'] += [key]\n",
    "    descriptions['full_name'] += [key]\n",
    "    descriptions['type'] += ['column']\n",
    "    descriptions['location'] += ['metadata']\n",
    "    descriptions['description'] += [value]\n",
    "    descriptions['data_type'] += ['INTEGER']\n",
    "\n",
    "for key, value in loans_data_columns_description.items():\n",
    "    if key in df.columns:\n",
    "        descriptions['name'] += [key]\n",
    "        descriptions['full_name'] += [column_full_names[key]]\n",
    "        descriptions['type'] += ['column']\n",
    "        descriptions['location'] += ['loans_data']\n",
    "        descriptions['description'] += [value]\n",
    "        descriptions['data_type'] += [map_dtype_to_sqlite(str(df[key].dtype))]\n",
    "    \n",
    "for key, value in descriptions_columns_description.items():\n",
    "    descriptions['name'] += [key]\n",
    "    descriptions['full_name'] += [key]\n",
    "    descriptions['type'] += ['column']\n",
    "    descriptions['location'] += ['descriptions']\n",
    "    descriptions['description'] += [value]\n",
    "    descriptions['data_type'] += ['TEXT']\n",
    "\n",
    "descriptions = pd.DataFrame(descriptions)\n",
    "\n",
    "print(\"Done With Creating Descriptions DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Database File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQLite database\n",
    "print(f\"Connecting to Database at {sqlite_file}\")\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "print(\"Creating Queries\")\n",
    "# Query to delete loans_data if it exists\n",
    "drop_loans_data_query = 'DROP TABLE IF EXISTS loans_data'\n",
    "# Query to delete metadata if it exists\n",
    "drop_metadata_query = 'DROP TABLE IF EXISTS metadata'\n",
    "# Query to delete descriptions if it exists\n",
    "drop_descriptions_query = 'DROP TABLE IF EXISTS descriptions'\n",
    "\n",
    "# Query to create a table for the loans data\n",
    "create_loans_data_table_query = 'CREATE TABLE loans_data (' + ', '.join([f\"\\\"{col}\\\" {col_type}\" for col, col_type in zip(df.columns, [map_dtype_to_sqlite(str(df[col].dtype)) for col in df.columns])]) + ')'\n",
    "# Query to create a table for the metadata\n",
    "create_metadata_table_query = 'CREATE TABLE metadata (' + ', '.join([f\"\\\"{col}\\\" TEXT\" for col in metadata.columns]) + ')'\n",
    "# Query to create a table for the descriptions\n",
    "create_descriptions_table_query = 'CREATE TABLE descriptions (' + ', '.join([f\"\\\"{col}\\\" TEXT\" for col in descriptions.columns]) + ')'\n",
    "\n",
    "print(\"Dropping old tables and creating new ones\")\n",
    "# Drops and creates the tables\n",
    "conn.execute(drop_loans_data_query)\n",
    "conn.execute(create_loans_data_table_query)\n",
    "conn.execute(drop_metadata_query)\n",
    "conn.execute(create_metadata_table_query)\n",
    "conn.execute(drop_descriptions_query)\n",
    "conn.execute(create_descriptions_table_query)\n",
    "\n",
    "print(\"Loading data into tables\")\n",
    "# Insert data from DataFrame to the SQLite table\n",
    "df.to_sql('loans_data', conn, if_exists='replace', index=False)\n",
    "metadata.to_sql('metadata', conn, if_exists='replace', index=False)\n",
    "descriptions.to_sql('descriptions', conn, if_exists='replace', index=False)\n",
    "\n",
    "conn.close()\n",
    "print(\"Done With Constructing the Database File\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinancialEnv",
   "language": "python",
   "name": "financialenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
