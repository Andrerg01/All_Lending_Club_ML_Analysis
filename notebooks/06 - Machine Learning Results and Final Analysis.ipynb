{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Training and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is dependent on the execution of `scripts/03 - data_preprocessing.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Defining Classes\")\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.log_dir = config['logging']['out_dir']\n",
    "        self.tag = config['base']['tag']\n",
    "        self.file_path = os.path.join('outputs', self.tag, self.log_dir, 'log.txt')\n",
    "        self.verbose = config['logging']['verbose']\n",
    "        \n",
    "    def log(self, message):\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        datetime_string = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_message = f\"{datetime_string}: {message}\"\n",
    "        if self.verbose:\n",
    "            print(log_message)\n",
    "        with open(self.file_path, \"a\") as f:\n",
    "            f.write(f'{log_message}\\n')\n",
    "\n",
    "print(\"Defining Functions\")\n",
    "\n",
    "def loan_status_to_int(status):\n",
    "    if status == 'Charged Off':\n",
    "        return 0\n",
    "    if status == 'Fully Paid':\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def bestbandwidth(data):\n",
    "    return 1.06*np.std(data)*len(data)**(-1/5)\n",
    "\n",
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        return f\"Created directory: {directory}\"\n",
    "    else:\n",
    "        return f\"Directory already exists: {directory}\"\n",
    "\n",
    "def transform_type(sqlite_type):\n",
    "    if sqlite_type == 'INTEGER':\n",
    "        return 'int'\n",
    "    if sqlite_type == 'REAL':\n",
    "        return 'float'\n",
    "    if sqlite_type == 'TEXT':\n",
    "        return 'object'\n",
    "\n",
    "def map_dtype_to_sqlite(col_type):\n",
    "    if col_type.startswith('int') or col_type == 'bool':\n",
    "        return 'INTEGER'\n",
    "    elif col_type.startswith('float'):\n",
    "        return 'REAL'\n",
    "    else:  # Default case, particularly for 'object' and other unhandled types\n",
    "        return 'TEXT'\n",
    "\n",
    "print(\"Reading Config File\")\n",
    "\n",
    "config_file_path = '../config/config.yml'\n",
    "\n",
    "root_path = '..'\n",
    "\n",
    "print(f\"Reading Config File {config_file_path}\")\n",
    "with open(config_file_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Defining Variables and Creating Directories\")\n",
    "\n",
    "sqlite_file = config['data']['output_sqlite']\n",
    "\n",
    "tag = config['base']['tag']\n",
    "\n",
    "git_repo = config['base']['git_repo']\n",
    "\n",
    "fontsize = config['plotting']['fontsize']\n",
    "figsize_x = config['plotting']['figure_xsize']\n",
    "figsize_y = config['plotting']['figure_ysize']\n",
    "\n",
    "optimization_sample_size = config['machine_learning']['optimization_sample_size']\n",
    "\n",
    "out_dir_figures = f\"outputs/{tag}/figures\"\n",
    "out_dir_stats = f\"outputs/{tag}/stats\"\n",
    "out_dir_log = f\"outputs/{tag}/log\"\n",
    "out_dir_models = f\"outputs/{tag}/models\"\n",
    "\n",
    "sqlite_file = os.path.join(f'{root_path}/outputs/{tag}/data/{sqlite_file}')\n",
    "out_dir_figures = os.path.join(root_path, out_dir_figures)\n",
    "out_dir_stats = os.path.join(root_path, out_dir_stats)\n",
    "out_dir_log = os.path.join(root_path, out_dir_log)\n",
    "out_dir_models = os.path.join(root_path, out_dir_log)\n",
    "\n",
    "columns_of_interest = config['base']['columns_of_interest']\n",
    "\n",
    "print(\"Done with initial setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Full and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Data\")\n",
    "# Defining the connection to the database\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "# Loading data into dataframe\n",
    "data_fetch_query = f\"\"\"SELECT * \n",
    "                       FROM loans_data_ML\n",
    "                       ORDER BY RANDOM()\"\"\"\n",
    "\n",
    "loans_data = pd.read_sql_query(data_fetch_query, conn, index_col='id')\n",
    "\n",
    "# Closing connection\n",
    "conn.close()\n",
    "\n",
    "print(\"Separating sample data for ML hyperparameter optimization, sample data will be balanced always, so will training data.\")\n",
    "\n",
    "sample_data = loans_data.sample(optimization_sample_size)\n",
    "\n",
    "print(\"Done with Loading Full and Sample Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Hyperparameter Optimization and Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # Since we're setting this at the sys level, it should not be overridden\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # Also affect subprocesses\n",
    "   \n",
    "X_opt, y_opt = sample_data.drop('loan_status', axis='columns').values, sample_data['loan_status'].values\n",
    "\n",
    "X, y = balanced_loans_data.drop('loan_status', axis='columns').values, balanced_loans_data['loan_status'].values\n",
    "\n",
    "ML_columns = balanced_loans_data.drop('loan_status', axis='columns').columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Through exploration we find that:\n",
    " - `solver = 'lbfgs'`\n",
    "   - `C < 1e-6` causes an overfit, where the model only predicts defaults\n",
    "   - Optimal `C` seems to be any value above 1e-5. giving around 56% accuracy.\n",
    " - `solver = 'liblinear'`\n",
    "   - `l2` penalty is much faster than `l1`\n",
    "   - Plateaus around `C 1e-2`, not much change if fitting time\n",
    "   - Best accuract around ~65%\n",
    " - `solver = 'sag'`\n",
    "   - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(class_weight='balanced', max_iter = 1000000)\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',  # Default scorer\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1_score': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "allowed_penalties = {\n",
    "    'lbfgs': ['l2'],\n",
    "    'liblinear': ['l2'],\n",
    "    'newton-cg': ['l2']\n",
    "}\n",
    "\n",
    "for i, solver in enumerate(allowed_penalties.keys()):\n",
    "    print(f\"Optimizing for {solver} | {i}/{len(allowed_penalties.keys())}\")\n",
    "    param_grid = {\n",
    "        'C': np.logspace(-10, 10, num = 50),\n",
    "        'solver': [solver],\n",
    "        'penalty': allowed_penalties[solver]\n",
    "        }\n",
    "    grid_search = GridSearchCV(logistic_regression, param_grid, cv=5, verbose=10, scoring=scoring, refit='f1_score')\n",
    "\n",
    "    grid_search.fit(X_opt, y_opt)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    results['ML_model'] = [\"LogisticRegression\"]*len(results)\n",
    "    if cummulative_results is None:\n",
    "        cummulative_results = results.copy()\n",
    "    else:\n",
    "        cummulative_results = pd.concat([cummulative_results, results])\n",
    "\n",
    "for param in ['C', 'solver', 'penalty']:\n",
    "    cummulative_results[f'param_{param}'] = [row['params'][param] for _, row in cummulative_results.iterrows()]\n",
    "\n",
    "fig, ax = plt.subplots(5, figsize=[10, 25/1.62])\n",
    "\n",
    "best_bw_time = bestbandwidth(cummulative_results['mean_fit_time'])\n",
    "min_time, max_time = cummulative_results['mean_fit_time'].min(), cummulative_results['mean_fit_time'].max()\n",
    "nBins_time = int((max_time - min_time)/best_bw_time)\n",
    "bins_time = np.linspace(min_time, max_time, num = nBins_time)\n",
    "\n",
    "for solver in cummulative_results['param_solver'].unique():\n",
    "    for penalty in cummulative_results['param_penalty'].unique():\n",
    "        label = f'Solver: {solver}, Penalty: {penalty}'\n",
    "        mask_solver = cummulative_results['param_solver'].apply(lambda x: str(x) == solver)\n",
    "        mask_panelty = cummulative_results['param_penalty'].apply(lambda x: str(x) == penalty)\n",
    "        results_temp = cummulative_results[mask_solver & mask_panelty]\n",
    "        \n",
    "        Cs = results_temp['param_C']\n",
    "        mean_scores = results_temp['mean_test_accuracy']\n",
    "        std_scores = results_temp['std_test_accuracy']\n",
    "        ax[0].errorbar(Cs, mean_scores, yerr=std_scores, fmt='o', label=label)\n",
    "        ax[0].scatter(Cs, mean_scores, label=f'Penalty: {penalty}')\n",
    "\n",
    "        mean_scores = results_temp['mean_test_precision']\n",
    "        std_scores = results_temp['std_test_precision']\n",
    "        ax[1].errorbar(Cs, mean_scores, yerr=std_scores, fmt='o', label=label)\n",
    "        ax[1].scatter(Cs, mean_scores, label=f'Penalty: {penalty}')\n",
    "\n",
    "        mean_scores = results_temp['mean_test_recall']\n",
    "        std_scores = results_temp['std_test_recall']\n",
    "        ax[2].errorbar(Cs, mean_scores, yerr=std_scores, fmt='o', label=label)\n",
    "        ax[2].scatter(Cs, mean_scores, label=f'Penalty: {penalty}')\n",
    "\n",
    "        mean_scores = results_temp['mean_test_f1_score']\n",
    "        std_scores = results_temp['std_test_f1_score']\n",
    "        ax[3].errorbar(Cs, mean_scores, yerr=std_scores, fmt='o', label=label)\n",
    "        ax[3].scatter(Cs, mean_scores, label=f'Penalty: {penalty}')\n",
    "\n",
    "        ax[4].hist(results_temp['mean_fit_time'], alpha=0.1, edgecolor = 'k', bins=bins_time, label=label)\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_xlabel('C (Inverse of Regularization Strength)')\n",
    "ax[0].set_ylabel('Mean Accuracy Score')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('C (Inverse of Regularization Strength)')\n",
    "ax[1].set_ylabel('Mean Precision Score')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].set_xscale('log')\n",
    "ax[2].set_xlabel('C (Inverse of Regularization Strength)')\n",
    "ax[2].set_ylabel('Mean Recall Score')\n",
    "ax[2].legend()\n",
    "\n",
    "ax[3].set_xscale('log')\n",
    "ax[3].set_xlabel('C (Inverse of Regularization Strength)')\n",
    "ax[3].set_ylabel('Mean F1 Score')\n",
    "ax[3].legend()\n",
    "\n",
    "ax[4].set_xlabel(\"Fitting Time (s)\")\n",
    "ax[4].set_ylabel('Count')\n",
    "ax[4].legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_results = cummulative_results.replace('None', 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_results.sort_values('mean_test_f1_score', inplace=True)\n",
    "cummulative_results.dropna(inplace=True)\n",
    "print(\"Loading best parameters and defining\")\n",
    "# Initialize the model\n",
    "best_C = cummulative_results.iloc[-1]['param_C']\n",
    "best_solver = cummulative_results.iloc[-1]['param_solver']\n",
    "best_penalty = cummulative_results.iloc[-1]['param_penalty']\n",
    "\n",
    "print(f\"Best Parameters: \\nC: {best_C}\\nsolver: {best_solver}\\npenalty {best_penalty}\")\n",
    "logistic_model = LogisticRegression(class_weight='balanced',\n",
    "                                    max_iter=100000,\n",
    "                                    C=best_C, \n",
    "                                    solver=best_solver,\n",
    "                                    penalty=best_penalty)\n",
    "\n",
    "print(\"Fitting the model to the training data\")\n",
    "# Train the model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Making predictions with the fit model\")\n",
    "# Make predictions\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "print(\"Computing cross-validations\")\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    logistic_model, X_train, y_train, cv=5, n_jobs=-1, \n",
    "    train_sizes=np.linspace(.1, 1.0, 10),\n",
    "    verbose=10)\n",
    "\n",
    "print(\"Computing mean scores\")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "coefficients = np.abs(logistic_model.coef_[0])\n",
    "\n",
    "# Combining feature names and their corresponding coefficients\n",
    "features, coefs = zip(*sorted(zip(ML_columns, coefficients), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "features = np.array(features)\n",
    "coefs = np.array(coefs)\n",
    "\n",
    "features = features[coefs > 0]\n",
    "coefs = coefs[coefs > 0]\n",
    "\n",
    "print(\"Making Plots\")\n",
    "fig, ax = plt.subplots(1, 3, figsize = [20*1.62, 7.5])\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizing the confusion matrix\n",
    "\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('Confusion Matrix')\n",
    "ax[0].set_ylabel('Actual Labels')\n",
    "ax[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "ax[1].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "ax[1].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "ax[1].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "ax[1].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "ax[1].set_title('Learning Curve')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Score')\n",
    "ax[1].legend(loc=\"best\")\n",
    "\n",
    "\n",
    "sns.barplot(x=coefs, y=features, edgecolor=\"black\", ax=ax[2])\n",
    "ax[2].set_title(\"Correlation between Loan status and Numeric Features\")\n",
    "ax[2].set_xlabel('Correlation')\n",
    "ax[2].set_ylabel('Numerical Features')\n",
    "ax[2].tick_params(axis='y', labelsize=5)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "random_forest_classifier = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',  # Default scorer\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1_score': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [int(i) for i in np.logspace(2, 3, 5)],  # Fewer, more spaced out values\n",
    "    'criterion': [\"gini\", \"entropy\"],\n",
    "    'max_features': [\"sqrt\", \"log2\"],  # Adding a fraction\n",
    "    'max_depth': [None, 10, 30, 50],  # None means fully grown trees\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(random_forest_classifier,\n",
    "                           param_grid,\n",
    "                           cv=5,\n",
    "                           verbose=10,\n",
    "                           scoring=scoring,\n",
    "                           refit='recall',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_opt, y_opt)\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results['ML_model'] = [\"RandomForestClassifier\"]*len(results)\n",
    "\n",
    "for param in ['n_estimators', 'criterion', 'max_features']:\n",
    "    results[f'param_{param}'] = [row['params'][param] for _, row in results.iterrows()]\n",
    "\n",
    "if cummulative_results is None:\n",
    "    cummulative_results = results.copy()\n",
    "else:\n",
    "    cummulative_results = pd.concat([cummulative_results, results])\n",
    "\n",
    "fig, ax = plt.subplots(5, figsize=[10, 25/1.62])\n",
    "\n",
    "best_bw_time = bestbandwidth(results['mean_fit_time'])\n",
    "min_time, max_time = results['mean_fit_time'].min(), results['mean_fit_time'].max()\n",
    "nBins_time = int((max_time - min_time)/best_bw_time)\n",
    "bins_time = np.linspace(min_time, max_time, num = nBins_time)\n",
    "\n",
    "for criterion in results['param_criterion'].dropna().unique():\n",
    "    for max_features in results['param_max_features'].dropna().unique():\n",
    "        label = f'Criterion: {criterion}, Max Features: {max_features}'\n",
    "        mask_criterion = results['param_criterion'].apply(lambda x: str(x) == criterion)\n",
    "        mask_max_features = results['param_max_features'].apply(lambda x: str(x) == max_features)\n",
    "        results_temp = results[mask_criterion & mask_max_features].dropna()\n",
    "\n",
    "        n_estimators = results_temp['param_n_estimators']\n",
    "        mean_scores = results_temp['mean_test_accuracy']\n",
    "        std_scores = results_temp['std_test_accuracy']\n",
    "        ax[0].errorbar(n_estimators, mean_scores, yerr=std_scores, fmt='o')\n",
    "        ax[0].scatter(n_estimators, mean_scores, label=label)\n",
    "\n",
    "        mean_scores = results_temp['mean_test_precision']\n",
    "        std_scores = results_temp['std_test_precision']\n",
    "        ax[1].errorbar(n_estimators, mean_scores, yerr=std_scores, fmt='o')\n",
    "        ax[1].scatter(n_estimators, mean_scores, label=label)\n",
    "\n",
    "        mean_scores = results_temp['mean_test_recall']\n",
    "        std_scores = results_temp['std_test_recall']\n",
    "        ax[2].errorbar(n_estimators, mean_scores, yerr=std_scores, fmt='o')\n",
    "        ax[2].scatter(n_estimators, mean_scores, label=label)\n",
    "\n",
    "        mean_scores = results_temp['mean_test_f1_score']\n",
    "        std_scores = results_temp['std_test_f1_score']\n",
    "        ax[3].errorbar(n_estimators, mean_scores, yerr=std_scores, fmt='o')\n",
    "        ax[3].scatter(n_estimators, mean_scores, label=label)\n",
    "\n",
    "        ax[4].hist(results_temp['mean_fit_time'], alpha=0.1, edgecolor = 'k', bins=bins_time, label=label)\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_xlabel('Number of Estimators')\n",
    "ax[0].set_ylabel('Mean Accuracy Score')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('Number of Estimators')\n",
    "ax[1].set_ylabel('Mean Precision Score')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].set_xscale('log')\n",
    "ax[2].set_xlabel('Number of Estimators')\n",
    "ax[2].set_ylabel('Mean Recall Score')\n",
    "ax[2].legend()\n",
    "\n",
    "ax[4].set_xscale('log')\n",
    "ax[3].set_xlabel('Number of Estimators')\n",
    "ax[3].set_ylabel('Mean F1 Score')\n",
    "ax[3].legend()\n",
    "\n",
    "ax[4].set_xlabel(\"Fitting Time (s)\")\n",
    "ax[4].set_ylabel('Count')\n",
    "ax[4].legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('mean_test_recall', inplace=True)\n",
    "results.dropna(inplace=True)\n",
    "print(\"Loading best parameters and defining\")\n",
    "# Initialize the model\n",
    "best_n_estimators = results.iloc[-1]['param_n_estimators']\n",
    "best_criterion = results.iloc[-1]['param_criterion']\n",
    "best_max_features = results.iloc[-1]['param_max_features']\n",
    "best_max_depth = results.iloc[-1]['param_max_depth']\n",
    "best_min_samples_split = results.iloc[-1]['param_min_samples_split']\n",
    "best_min_samples_leaf = results.iloc[-1]['param_min_samples_leaf']\n",
    "best_bootstrap = results.iloc[-1]['param_bootstrap']\n",
    "\n",
    "print(f\"\"\"Best Parameters:\n",
    " - N Estimators: {best_n_estimators}\n",
    " - Criterion: {best_criterion}\n",
    " - Max_Features {best_max_features}\n",
    " - Max Depth {best_max_depth}\n",
    " - \"\"\")\n",
    "random_forest_classifier = RandomForestClassifier(class_weight='balanced',\n",
    "                                    n_estimators=best_n_estimators, \n",
    "                                    criterion=best_criterion,\n",
    "                                    max_features=best_max_features,\n",
    "                                    max_depth=best_max_depth,\n",
    "                                    min_samples_split=best_min_samples_split,\n",
    "                                    min_samples_leaf=best_min_samples_leaf,\n",
    "                                    bootstrap=best_bootstrap)\n",
    "\n",
    "print(\"Fitting the model to the training data\")\n",
    "# Train the model\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Making predictions with the fit model\")\n",
    "# Make predictions\n",
    "y_pred = random_forest_classifier.predict(X_test)\n",
    "\n",
    "# print(\"Computing cross-validations\")\n",
    "# train_sizes, train_scores, test_scores = learning_curve(\n",
    "#     random_forest_classifier, X, y, cv=5, n_jobs=-1, \n",
    "#     train_sizes=np.linspace(.1, 1.0, 10),\n",
    "#     verbose=10)\n",
    "\n",
    "print(\"Computing mean scores\")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "feature_importances = random_forest_classifier.feature_importances_\n",
    "\n",
    "features = np.array(ML_columns)  # Assuming ML_columns are your feature names\n",
    "importances = feature_importances\n",
    "\n",
    "# Sorting features by importance\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "sorted_features = features[sorted_indices]\n",
    "sorted_importances = importances[sorted_indices]\n",
    "\n",
    "print(\"Making Plots\")\n",
    "fig, ax = plt.subplots(1, 3, figsize = [20*1.62, 7.5])\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizing the confusion matrix\n",
    "\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('Confusion Matrix')\n",
    "ax[0].set_ylabel('Actual Labels')\n",
    "ax[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "ax[1].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "ax[1].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "ax[1].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "ax[1].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "ax[1].set_title('Learning Curve')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Score')\n",
    "ax[1].legend(loc=\"best\")\n",
    "\n",
    "sns.barplot(x=sorted_importances, y=sorted_features, edgecolor=\"black\", ax=ax[2])\n",
    "ax[2].set_title(\"Feature Importances in RandomForestClassifier\")\n",
    "ax[2].set_xlabel('Importance')\n",
    "ax[2].set_ylabel('Features')\n",
    "ax[2].tick_params(axis='y', labelsize=5)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "mlp_classifier = MLPClassifier()\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',  # Default scorer\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1_score': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50, 100), (50, 100, 50)],\n",
    "    'activation': ['relu'],  # Common choices for activation functions\n",
    "    'solver': ['adam'],  # Stochastic gradient descent and Adam optimizer\n",
    "    'alpha': [0.0015, 0.002, 0.0025],  # L2 penalty (regularization term) parameter\n",
    "    'learning_rate': ['constant'],  # Learning rate schedule\n",
    "    'max_iter': [125, 150, 175],  # Maximum number of iterations. The solver iterates until convergence or this number of iterations.\n",
    "}\n",
    "\n",
    "prod = 5\n",
    "for value in param_grid.values():\n",
    "    prod *= len(value)\n",
    "print(f\"Total Number of Fits: {prod}\")\n",
    "grid_search = GridSearchCV(mlp_classifier,\n",
    "                           param_grid,\n",
    "                           cv=5,\n",
    "                           verbose=False,\n",
    "                           scoring=scoring,\n",
    "                           refit='precision',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_opt, y_opt)\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results['ML_model'] = [\"MLPClassifier\"]*len(results)\n",
    "\n",
    "for param in param_grid.keys():\n",
    "    results[f'param_{param}'] = [row['params'][param] for _, row in results.iterrows()]\n",
    "\n",
    "if cummulative_results is None:\n",
    "    cummulative_results = results.copy()\n",
    "else:\n",
    "    cummulative_results = pd.concat([cummulative_results, results])\n",
    "\n",
    "fig, ax = plt.subplots(5, figsize=[10, 25/1.62])\n",
    "\n",
    "best_bw_time = bestbandwidth(results['mean_fit_time'])\n",
    "min_time, max_time = results['mean_fit_time'].min(), results['mean_fit_time'].max()\n",
    "nBins_time = int((max_time - min_time)/best_bw_time)\n",
    "bins_time = np.linspace(min_time, max_time, num = nBins_time)\n",
    "\n",
    "label = f'whateves for now'\n",
    "n_estimators = [len(r) for r in results['param_hidden_layer_sizes']]\n",
    "mean_scores = results['mean_test_accuracy']\n",
    "std_scores = results['std_test_accuracy']\n",
    "ax[0].errorbar(n_estimators, mean_scores, yerr=std_scores, fmt='o')\n",
    "ax[0].scatter(n_estimators, mean_scores, label=label)\n",
    "\n",
    "mean_scores = results['mean_test_precision']\n",
    "std_scores = results['std_test_precision']\n",
    "ax[1].errorbar(n_estimators, mean_scores, yerr=std_scores, fmt='o')\n",
    "ax[1].scatter(n_estimators, mean_scores, label=label)\n",
    "\n",
    "mean_scores = results['mean_test_recall']\n",
    "std_scores = results['std_test_recall']\n",
    "ax[2].errorbar(n_estimators, mean_scores, yerr=std_scores, fmt='o')\n",
    "ax[2].scatter(n_estimators, mean_scores, label=label)\n",
    "\n",
    "mean_scores = results['mean_test_f1_score']\n",
    "std_scores = results['std_test_f1_score']\n",
    "ax[3].errorbar(n_estimators, mean_scores, yerr=std_scores, fmt='o')\n",
    "ax[3].scatter(n_estimators, mean_scores, label=label)\n",
    "\n",
    "ax[4].hist(results['mean_fit_time'], alpha=0.1, edgecolor = 'k', bins=bins_time, label=label)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "ax[0].set_xlabel('Number of Estimators')\n",
    "ax[0].set_ylabel('Mean Accuracy Score')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_xlabel('Number of Estimators')\n",
    "ax[1].set_ylabel('Mean Precision Score')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].set_xlabel('Number of Estimators')\n",
    "ax[2].set_ylabel('Mean Recall Score')\n",
    "ax[2].legend()\n",
    "\n",
    "ax[3].set_xlabel('Number of Estimators')\n",
    "ax[3].set_ylabel('Mean F1 Score')\n",
    "ax[3].legend()\n",
    "\n",
    "ax[4].set_xlabel(\"Fitting Time (s)\")\n",
    "ax[4].set_ylabel('Count')\n",
    "ax[4].legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('mean_test_precision', inplace=True)\n",
    "results.dropna(inplace=True)\n",
    "print(\"Loading best parameters and defining\")\n",
    "# Initialize the model\n",
    "best_hidden_layer_sizes = results.iloc[-1]['param_hidden_layer_sizes']\n",
    "best_activation = results.iloc[-1]['param_activation']\n",
    "best_solver = results.iloc[-1]['param_solver']\n",
    "best_alpha = results.iloc[-1]['param_alpha']\n",
    "best_learning_rate = results.iloc[-1]['param_learning_rate']\n",
    "best_max_iter = results.iloc[-1]['param_max_iter']\n",
    "\n",
    "print(f\"\"\"Best Parameters:\n",
    " - Hidden Layers: {best_hidden_layer_sizes}\n",
    " - Activation: {best_activation}\n",
    " - Solver {best_solver}\n",
    " - Alpha {best_alpha}\n",
    " - Learning Rate {best_learning_rate}\n",
    " - Max Iterations {best_max_iter}\"\"\")\n",
    "\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=best_hidden_layer_sizes, \n",
    "                               activation=best_activation,\n",
    "                               solver=best_solver,\n",
    "                               alpha=best_alpha,\n",
    "                               learning_rate=best_learning_rate,\n",
    "                               max_iter=best_max_iter)\n",
    "\n",
    "print(\"Fitting the model to the training data\")\n",
    "\n",
    "# Train the model\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Making predictions with the fit model\")\n",
    "# Make predictions\n",
    "y_pred = mlp_classifier.predict(X_test)\n",
    "\n",
    "# print(\"Computing cross-validations\")\n",
    "# train_sizes, train_scores, test_scores = learning_curve(\n",
    "#     mlp_classifier, X, y, cv=5, n_jobs=-1, \n",
    "#     train_sizes=np.linspace(.1, 1.0, 10),\n",
    "#     verbose=10)\n",
    "\n",
    "print(\"Computing mean scores\")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "feature_importances = random_forest_classifier.feature_importances_\n",
    "\n",
    "features = np.array(ML_columns)  # Assuming ML_columns are your feature names\n",
    "importances = feature_importances\n",
    "\n",
    "# Sorting features by importance\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "sorted_features = features[sorted_indices]\n",
    "sorted_importances = importances[sorted_indices]\n",
    "\n",
    "print(\"Making Plots\")\n",
    "fig, ax = plt.subplots(1, 3, figsize = [20*1.62, 7.5])\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizing the confusion matrix\n",
    "\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('Confusion Matrix')\n",
    "ax[0].set_ylabel('Actual Labels')\n",
    "ax[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "ax[1].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "ax[1].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "ax[1].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "ax[1].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "ax[1].set_title('Learning Curve')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Score')\n",
    "ax[1].legend(loc=\"best\")\n",
    "\n",
    "sns.barplot(x=sorted_importances, y=sorted_features, edgecolor=\"black\", ax=ax[2])\n",
    "ax[2].set_title(\"Feature Importances in RandomForestClassifier\")\n",
    "ax[2].set_xlabel('Importance')\n",
    "ax[2].set_ylabel('Features')\n",
    "ax[2].tick_params(axis='y', labelsize=5)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulative_results[cummulative_results['ML_model'] == 'RandomForestClassifier'].sort_values('mean_test_recall').iloc[-1]['params']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical, Real\n",
    "\n",
    "df = pd.read_csv('../outputs/prototype/models/Cummulative_Results_GridSearchCV.csv', index_col=0)\n",
    "df.sort_values(['mean_test_f1_score'])\n",
    "\n",
    "model = df.sort_values(['mean_test_f1_score']).iloc[-1]['ML_model']\n",
    "params = eval(df.sort_values(['mean_test_f1_score']).iloc[-1]['params'])\n",
    "if model == 'RandomForestClassifier':\n",
    "    clf = RandomForestClassifier(bootstrap=params['bootstrap'],\n",
    "                                 class_weight=params['class_weight'],\n",
    "                                 criterion=params['criterion'],\n",
    "                                 max_features=params['max_features'])\n",
    "    search_spaces = {\n",
    "    'max_depth': Integer(int(params['max_depth']/2), int(params['max_depth']*1.5)),\n",
    "    'min_samples_leaf': Integer(int(params['min_samples_leaf']/2), int(params['min_samples_leaf']*1.5)),\n",
    "    'min_samples_split': Integer(int(params['min_samples_split']/2), int(params['min_samples_split']*1.5)),\n",
    "    'n_estimators': Integer(int(params['n_estimators']/2), int(params['n_estimators']*1.5))\n",
    "    }\n",
    "\n",
    "elif model == 'LogisticRegression':\n",
    "    print(\"Model: LogisticRegression\")\n",
    "    print(f\"Starting Parameters: {params}\")\n",
    "    clf = LogisticRegression(penalty=params['penalty'],\n",
    "                             solver=params['solver'])\n",
    "    \n",
    "    search_spaces = {\n",
    "    'C': Real(params['C']/2, params['C']*1.5)\n",
    "    }\n",
    "    \n",
    "print(f\"Model: {model}\")\n",
    "print(f\"Starting Parameters: {params}\")\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=clf,\n",
    "    search_spaces=search_spaces,\n",
    "    scoring=make_scorer(recall_score),\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "np.int = int\n",
    "bayes_search.fit(X_opt, y_opt)\n",
    "\n",
    "best_params = bayes_search.best_params_\n",
    "print(\"Best parameters to maximize recall: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('mean_test_recall', inplace=True)\n",
    "results.dropna(inplace=True)\n",
    "print(\"Loading best parameters and defining\")\n",
    "\n",
    "if model == 'RandomForestClassifier':\n",
    "    clf = RandomForestClassifier(bootstrap=params['bootstrap'],\n",
    "                             class_weight=params['class_weight'],\n",
    "                             criterion=params['criterion'],\n",
    "                             max_features=params['max_features'],\n",
    "                             max_depth=best_params['max_depth'],\n",
    "                             min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                             min_samples_split=best_params['min_samples_split'],\n",
    "                             n_estimators=best_params['n_estimators'],\n",
    "                             )\n",
    "elif model == 'LogisticRegression':\n",
    "    clf = LogisticRegression(penalty=params['penalty'],\n",
    "                             solver=params['solver'],\n",
    "                             C=best_params['C'])\n",
    "\n",
    "print(\"Fitting the model to the training data\")\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Making predictions with the fit model\")\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# print(\"Computing cross-validations\")\n",
    "# train_sizes, train_scores, test_scores = learning_curve(\n",
    "#     random_forest_classifier, X, y, cv=5, n_jobs=-1, \n",
    "#     train_sizes=np.linspace(.1, 1.0, 10),\n",
    "#     verbose=10)\n",
    "\n",
    "print(\"Computing mean scores\")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "features = np.array(ML_columns)  # Assuming ML_columns are your feature names\n",
    "importances = feature_importances\n",
    "\n",
    "# Sorting features by importance\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "sorted_features = features[sorted_indices]\n",
    "sorted_importances = importances[sorted_indices]\n",
    "\n",
    "print(\"Making Plots\")\n",
    "fig, ax = plt.subplots(1, 3, figsize = [20*1.62, 7.5])\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizing the confusion matrix\n",
    "\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('Confusion Matrix')\n",
    "ax[0].set_ylabel('Actual Labels')\n",
    "ax[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "ax[1].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "ax[1].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "ax[1].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "ax[1].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "ax[1].set_title('Learning Curve')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Score')\n",
    "ax[1].legend(loc=\"best\")\n",
    "\n",
    "sns.barplot(x=sorted_importances, y=sorted_features, edgecolor=\"black\", ax=ax[2])\n",
    "ax[2].set_title(\"Feature Importances in RandomForestClassifier\")\n",
    "ax[2].set_xlabel('Importance')\n",
    "ax[2].set_ylabel('Features')\n",
    "ax[2].tick_params(axis='y', labelsize=5)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = clf.predict_proba(loans_data.drop(['loan_status', 'Probability Default', 'Profit_or_Loss', 'Expected_Value'], axis='columns'))\n",
    "loans_data['Probability Default'] = [prob[1] for prob in probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshs = np.linspace(0, 1, 50)\n",
    "acc = np.array([0. for _ in range(len(threshs))])\n",
    "N = np.array([0 for _ in range(len(threshs))])\n",
    "for i, thresh in enumerate(threshs):\n",
    "    thresh_mask = (loans_data['Probability Default'] <= thresh)\n",
    "    loans_data_thresh = loans_data[thresh_mask]\n",
    "    if len(loans_data_thresh) == 0:\n",
    "        acc[i] = np.nan\n",
    "    else:\n",
    "        acc[i] = len(loans_data_thresh[loans_data_thresh['loan_status'] == 1])/len(loans_data_thresh)\n",
    "    N[i] = len(loans_data_thresh)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = [10, 10/1.62])\n",
    "ax.plot(threshs, acc)\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(threshs, N, width=np.diff(threshs)[0], color=None, edgecolor='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the connection to the database\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "# Loading data into dataframe\n",
    "data_fetch_query = f\"\"\"SELECT id, total_pymnt\n",
    "                       FROM loans_data\"\"\"\n",
    "\n",
    "loans_data_paymnts = pd.read_sql_query(data_fetch_query, conn)\n",
    "\n",
    "# Closing connection\n",
    "conn.close()\n",
    "\n",
    "loans_data_paymnts = loans_data_paymnts[loans_data_paymnts['id'].apply(lambda x: x in loans_data.index)]\n",
    "\n",
    "combined_data = pd.merge(loans_data, loans_data_paymnts, on='id', how='inner')\n",
    "combined_data['Profit_or_Loss'] = combined_data['total_pymnt'] - combined_data['loan_amnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_profit(df, threshold):\n",
    "    # Predicted defaults based on threshold\n",
    "    return df[df['Probability Default'] <= threshold]['Profit_or_Loss'].sum()\n",
    "\n",
    "actual_profit = combined_data['Profit_or_Loss'].sum()\n",
    "# Evaluate profits at various thresholds\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "profits = np.array([calculate_profit(combined_data, thresh) for thresh in thresholds])\n",
    "Ns = np.array([len(combined_data[combined_data['Probability Default'] <= thresh]) for thresh in thresholds])\n",
    "\n",
    "# Find the optimal threshold\n",
    "max_profit = profits.max()\n",
    "optimal_threshold = thresholds[profits.argmax()]\n",
    "print(f\"Maximum profit of {max_profit} is achieved at a threshold of {optimal_threshold:.2f}\")\n",
    "\n",
    "# Plot the profit curve using fig, ax\n",
    "fig, ax = plt.subplots(figsize=(10, 10/1.62))\n",
    "ax.plot(thresholds, profits, label='Profit')\n",
    "ax.axvline(optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold: {optimal_threshold:.2f}')\n",
    "ax.axhline(actual_profit, color='blue', linestyle='--', label='Unaltered Profit')\n",
    "ax.set_xlabel('Probability Default Threshold')\n",
    "ax.set_ylabel('Total Profit Considering Only Loans Below Threshold')\n",
    "ax.set_title('Profitability Considering Only Loans Below Threshold')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(thresholds, Ns, width=np.diff(thresholds)[0], color=None, edgecolor='k', alpha=0.1)\n",
    "ax2.set_ylabel('Number of Loans With Probability Default Below Threshold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the profit curve using fig, ax\n",
    "fig, ax = plt.subplots(figsize=(10, 10/1.62))\n",
    "\n",
    "relative_profits = profits/actual_profit\n",
    "\n",
    "max_relative_profit = relative_profits.max()\n",
    "optimal_threshold = thresholds[profits.argmax()]\n",
    "\n",
    "ax.plot(thresholds, relative_profits, label='Profit')\n",
    "ax.axvline(optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold: {optimal_threshold:.2f}')\n",
    "ax.axhline(1, color='blue', linestyle='--', label='Unaltered Profit')\n",
    "ax.set_xlabel('Probability Default Threshold')\n",
    "ax.set_ylabel('Total Profit Considering Only Loans Below Threshold')\n",
    "ax.set_title('Profitability Considering Only Loans Below Threshold')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(thresholds, Ns, width=np.diff(thresholds)[0], color=None, edgecolor='k', alpha=0.1)\n",
    "ax2.set_ylabel('Number of Loans With Probability Default Below Threshold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    combined_data.drop(['loan_status'], axis=1),\n",
    "    combined_data['loan_status'], test_size=0.2, random_state=42)\n",
    "\n",
    "df_test = X_test.join(y_test)\n",
    "\n",
    "def calculate_profit(df, threshold):\n",
    "    # Predicted defaults based on threshold\n",
    "    return df[df['Probability Default'] <= threshold]['Profit_or_Loss'].sum()\n",
    "\n",
    "actual_profit = df_test['Profit_or_Loss'].sum()\n",
    "# Evaluate profits at various thresholds\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "profits = np.array([calculate_profit(df_test, thresh) for thresh in thresholds])\n",
    "Ns = np.array([len(df_test[df_test['Probability Default'] <= thresh]) for thresh in thresholds])\n",
    "\n",
    "# Find the optimal threshold\n",
    "max_profit = profits.max()\n",
    "optimal_threshold = thresholds[profits.argmax()]\n",
    "print(f\"Maximum profit of {max_profit} is achieved at a threshold of {optimal_threshold:.2f}\")\n",
    "\n",
    "# Plot the profit curve using fig, ax\n",
    "fig, ax = plt.subplots(figsize=(10, 10/1.62))\n",
    "ax.plot(thresholds, profits, label='Profit')\n",
    "ax.axvline(optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold: {optimal_threshold:.2f}')\n",
    "ax.axhline(actual_profit, color='blue', linestyle='--', label='Unaltered Profit')\n",
    "ax.set_xlabel('Probability Default Threshold')\n",
    "ax.set_ylabel('Total Profit Considering Only Loans Below Threshold')\n",
    "ax.set_title('Profitability Considering Only Loans Below Threshold')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(thresholds, Ns, width=np.diff(thresholds)[0], color=None, edgecolor='k', alpha=0.1)\n",
    "ax2.set_ylabel('Number of Loans With Probability Default Below Threshold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SupportVectorClassifier\n",
    "\n",
    "def string_to_dict(string_dict):\n",
    "    tuple_list_str = string_dict.replace(\"OrderedDict\", \"\")\n",
    "    # Safely evaluate the string to a list of tuples using literal_eval\n",
    "    tuple_list = ast.literal_eval(tuple_list_str)\n",
    "    # Convert the list of tuples to an OrderedDict\n",
    "    actual_ordered_dict = OrderedDict(tuple_list)\n",
    "    # Convert OrderedDict to a regular dict\n",
    "    params = dict(actual_ordered_dict)\n",
    "    return params\n",
    "\n",
    "df = pd.read_csv('../outputs/2023_12_19/stats/Cummulative_Results_BayesSearchCV.csv', index_col=0)\n",
    "\n",
    "# model = df[df['ML_model'] == 'RandomForestClassifier'].sort_values('mean_test_score').iloc[-1]['ML_model']\n",
    "# params = string_to_dict(df[df['ML_model'] == 'RandomForestClassifier'].sort_values('mean_test_score').iloc[-1]['params'])\n",
    "model = 'LogisticRegression'\n",
    "params = string_to_dict(\"\"\"OrderedDict([('C', 5594575.138586148),\n",
    "             ('class_weight', 'balanced'),\n",
    "             ('max_iter', 1000000),\n",
    "             ('penalty', 'l2'),\n",
    "             ('solver', 'lbfgs')])\"\"\")\n",
    "\n",
    "print(\"Loading best parameters and defining\")\n",
    "\n",
    "if model == 'RandomForestClassifier':\n",
    "    clf = RandomForestClassifier(**params)\n",
    "elif model == 'LogisticRegression':\n",
    "    clf = LogisticRegression(**params)\n",
    "elif model == 'SupportVectorClassifier':\n",
    "    clf = SupportVectorClassifier(**params)\n",
    "\n",
    "print(\"Fitting the model to the training data\")\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Making predictions with the fit model\")\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# print(\"Computing cross-validations\")\n",
    "# train_sizes, train_scores, test_scores = learning_curve(\n",
    "#     random_forest_classifier, X, y, cv=5, n_jobs=-1, \n",
    "#     train_sizes=np.linspace(.1, 1.0, 10),\n",
    "#     verbose=False)\n",
    "\n",
    "# print(\"Computing mean scores\")\n",
    "# train_scores_mean = np.mean(train_scores, axis=1)\n",
    "# train_scores_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# test_scores_mean = np.mean(test_scores, axis=1)\n",
    "# test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "if model == 'RandomForestClassifier':\n",
    "    feature_importances = clf.feature_importances_\n",
    "\n",
    "    features = np.array(ML_columns)  # Assuming ML_columns are your feature names\n",
    "    importances = feature_importances\n",
    "\n",
    "    # Sorting features by importance\n",
    "    sorted_indices = np.argsort(importances)[::-1]\n",
    "    sorted_features = features[sorted_indices]\n",
    "    sorted_importances = importances[sorted_indices]\n",
    "elif model == 'LogisticRegression':\n",
    "    # Logistic Regression coefficients\n",
    "    coefficients = clf.coef_[0]  # Assuming clf is your Logistic Regression model\n",
    "\n",
    "    features = np.array(ML_columns)  # Assuming ML_columns are your feature names\n",
    "    importances = np.abs(coefficients)\n",
    "\n",
    "    # Sorting features by the absolute value of coefficients\n",
    "    sorted_indices = np.argsort(np.abs(importances))[::-1]\n",
    "    sorted_features = features[sorted_indices]\n",
    "    sorted_importances = importances[sorted_indices]\n",
    "elif model == 'SupportVectorClassifier':\n",
    "    if clf.kernel == 'linear':\n",
    "        # SVC coefficients for linear kernel\n",
    "        coefficients = clf.coef_[0]  # Assuming clf is your SVC model\n",
    "\n",
    "        features = np.array(ML_columns)  # Assuming ML_columns are your feature names\n",
    "        importances = np.abs(coefficients)\n",
    "\n",
    "        # Sorting features by the absolute value of coefficients\n",
    "        sorted_indices = np.argsort(importances)[::-1]\n",
    "        sorted_features = features[sorted_indices]\n",
    "        sorted_importances = importances[sorted_indices]\n",
    "    else:\n",
    "        # For non-linear kernels, feature importances are not directly available\n",
    "        sorted_features = None\n",
    "        sorted_importances = None\n",
    "        print(\"Feature importances are not available for non-linear kernels in SVC\")\n",
    "clear_output(wait=True)\n",
    "\n",
    "print(\"Making Plots\")\n",
    "fig, ax = plt.subplots(1, 3, figsize = [20*1.62, 7.5])\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalizing the confusion matrix\n",
    "\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('Confusion Matrix')\n",
    "ax[0].set_ylabel('Actual Labels')\n",
    "ax[0].set_xlabel('Predicted Labels')\n",
    "\n",
    "ax[1].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "ax[1].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "ax[1].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "ax[1].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "ax[1].set_title('Learning Curve')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Score')\n",
    "ax[1].legend(loc=\"best\")\n",
    "\n",
    "sns.barplot(x=sorted_importances, y=sorted_features, edgecolor=\"black\", ax=ax[2])\n",
    "ax[2].set_title(\"Feature Importances\")\n",
    "ax[2].set_xlabel('Importance Value')\n",
    "ax[2].set_ylabel('Features')\n",
    "ax[2].tick_params(axis='y', labelsize=5)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = clf.predict_proba(loans_data.drop(['loan_status', 'Default Probability', 'Probability Default', 'Profit_or_Loss', 'Expected_Value'], axis='columns'))\n",
    "loans_data['Default Probability'] = [prob[1] for prob in probabilities]\n",
    "\n",
    "threshs = np.linspace(0, 1, 100)\n",
    "prop = np.array([0. for _ in range(len(threshs))])\n",
    "N = np.array([0 for _ in range(len(threshs))])\n",
    "N_defaulted = np.array([0 for _ in range(len(threshs))])\n",
    "N_paid = np.array([0 for _ in range(len(threshs))])\n",
    "for i, thresh in enumerate(threshs):\n",
    "    thresh_mask = (loans_data['Default Probability'] <= thresh)\n",
    "    loans_data_thresh = loans_data[thresh_mask]\n",
    "    if len(loans_data_thresh) == 0:\n",
    "        prop[i] = np.nan\n",
    "    else:\n",
    "        prop[i] = len(loans_data_thresh[loans_data_thresh['loan_status'] == 1])/len(loans_data_thresh)\n",
    "    N[i] = len(loans_data_thresh)\n",
    "    N_defaulted[i] = len(loans_data_thresh[loans_data_thresh['loan_status'] == 1])\n",
    "    N_paid[i] = len(loans_data_thresh[loans_data_thresh['loan_status'] == 0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = [10, 10/1.62])\n",
    "ax.plot(threshs, prop, label='Proportion of Defaulted Loans')\n",
    "ax.set_xlabel('Threshold on Default Probability')\n",
    "ax.set_ylabel('Proportion of Defaulted Loans Below Threshold')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "# ax2.bar(threshs, N, width=np.diff(threshs)[0], color=None, edgecolor='k', alpha=0.1)\n",
    "ax2.bar(threshs, N_defaulted, width=np.diff(threshs)[0], color='Red', edgecolor='k', alpha=0.2, label='Defaulted')\n",
    "ax2.bar(threshs, N_paid, width=np.diff(threshs)[0], color='Blue', edgecolor='k', alpha=0.2, label='Paid Off', bottom=N_defaulted)\n",
    "ax2.set_ylabel(\"Number of Loans Below Threshold\")\n",
    "ax2.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_file = '../outputs/2023_12_19/data/All_Lending_Club_Loan_2007_2018.sqlite'\n",
    "# Defining the connection to the database\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "# Loading data into dataframe\n",
    "data_fetch_query = f\"\"\"SELECT id, total_pymnt\n",
    "                       FROM loans_data\"\"\"\n",
    "\n",
    "loans_data_paymnts = pd.read_sql_query(data_fetch_query, conn)\n",
    "\n",
    "# Closing connection\n",
    "conn.close()\n",
    "\n",
    "loans_data_paymnts = loans_data_paymnts[loans_data_paymnts['id'].apply(lambda x: x in loans_data.index)]\n",
    "\n",
    "combined_data = pd.merge(loans_data, loans_data_paymnts, on='id', how='inner')\n",
    "combined_data['Profit_or_Loss'] = combined_data['total_pymnt'] - combined_data['loan_amnt']\n",
    "\n",
    "def calculate_profit(df, threshold):\n",
    "    # Predicted defaults based on threshold\n",
    "    return df[df['Default Probability'] <= threshold]['Profit_or_Loss'].sum()\n",
    "\n",
    "actual_profit = combined_data['Profit_or_Loss'].sum()\n",
    "# Evaluate profits at various thresholds\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "profits = np.array([calculate_profit(combined_data, thresh) for thresh in thresholds])\n",
    "Ns = np.array([len(combined_data[combined_data['Default Probability'] <= thresh]) for thresh in thresholds])\n",
    "Ns_defaulted = np.array([len(combined_data[(combined_data['Default Probability'] <= thresh) & (combined_data['loan_status'] == 1)]) for thresh in thresholds])\n",
    "Ns_paid = np.array([len(combined_data[(combined_data['Default Probability'] <= thresh) & (combined_data['loan_status'] == 0)]) for thresh in thresholds])\n",
    "\n",
    "# Find the optimal threshold\n",
    "max_profit = profits.max()\n",
    "optimal_threshold = thresholds[profits.argmax()]\n",
    "print(f\"Maximum profit of {max_profit} is achieved at a threshold of {optimal_threshold:.2f}\")\n",
    "\n",
    "# Plot the profit curve using fig, ax\n",
    "fig, ax = plt.subplots(figsize=(10, 10/1.62))\n",
    "ax.plot(thresholds, profits, label='Profit')\n",
    "ax.axvline(optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold: {optimal_threshold:.2f}')\n",
    "ax.axhline(actual_profit, color='blue', linestyle='--', label='Unaltered Profit')\n",
    "ax.set_xlabel('Default Probability Threshold')\n",
    "ax.set_ylabel('Total Profit Considering Only Loans Below Threshold')\n",
    "ax.set_title('Profitability Considering Only Loans Below Threshold')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "# ax2.bar(thresholds, Ns, width=np.diff(thresholds)[0], color=None, edgecolor='k', alpha=0.1)\n",
    "ax2.bar(thresholds, Ns_defaulted, width=np.diff(thresholds)[0], color='Red', edgecolor='k', alpha=0.2, label='Defaulted')\n",
    "ax2.bar(thresholds, Ns_paid, width=np.diff(thresholds)[0], color='Blue', edgecolor='k', alpha=0.2, label='Paid Off', bottom=Ns_defaulted)\n",
    "ax2.set_ylabel('Number of Loans With Default Probability Below Threshold')\n",
    "ax2.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_profit = profits/actual_profit\n",
    "\n",
    "# Find the optimal threshold\n",
    "max_relative_profit = relative_profit.max()\n",
    "optimal_threshold = thresholds[relative_profit.argmax()]\n",
    "print(f\"Maximum Relative Profit of {max_relative_profit} is achieved at a threshold of {optimal_threshold:.2f}\")\n",
    "\n",
    "# Plot the profit curve using fig, ax\n",
    "fig, ax = plt.subplots(figsize=(10, 10/1.62))\n",
    "ax.plot(thresholds, relative_profit, label='Profit')\n",
    "ax.axvline(optimal_threshold, color='red', linestyle='--', label=f'Optimal Threshold: {optimal_threshold:.2f}')\n",
    "ax.axhline(1, color='blue', linestyle='--', label='Unaltered Profit')\n",
    "ax.set_xlabel('Default Probability Threshold')\n",
    "ax.set_ylabel('Total Profit Considering Only Loans Below Threshold')\n",
    "ax.set_title('Profitability Considering Only Loans Below Threshold')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "# ax2.bar(thresholds, Ns, width=np.diff(thresholds)[0], color=None, edgecolor='k', alpha=0.1)\n",
    "ax2.bar(thresholds, Ns_defaulted, width=np.diff(thresholds)[0], color='Red', edgecolor='k', alpha=0.2, label='Defaulted')\n",
    "ax2.bar(thresholds, Ns_paid, width=np.diff(thresholds)[0], color='Blue', edgecolor='k', alpha=0.2, label='Paid Off', bottom=Ns_defaulted)\n",
    "ax2.set_ylabel('Number of Loans With Probability Default Below Threshold')\n",
    "ax2.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../outputs/prototype/stats/Cummulative_Results_BayesSearchCV.csv', index_col=0)\n",
    "\n",
    "df['params'] = df['params'].apply(string_to_dict)\n",
    "\n",
    "best_bw = bestbandwidth(df['mean_test_score'].values)\n",
    "maxX = df['mean_test_score'].max()\n",
    "minX = df['mean_test_score'].min()\n",
    "nBins = int((maxX - minX)/best_bw)\n",
    "bins = np.linspace(minX, maxX, nBins)\n",
    "fig, ax = plt.subplots(figsize=[10, 10/1.62])\n",
    "for algorithm in df['ML_model'].unique():\n",
    "    ax.hist(df[df['ML_model'] == algorithm]['mean_test_score'], label=algorithm, bins=bins, histtype='step')\n",
    "ax.set_xlabel(\"Mean Test Score\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Distribution of Test Scores in Bayesian Search For Each Machine Learning Algorithm\")\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('../outputs/prototype/stats/best_model.pkl', 'rb') as f:\n",
    "    model = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../outputs/prototype/stats/Cummulative_Results_BayesSearchCV.csv', index_col=0)\n",
    "df.sort_values('mean_test_score').iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LendingClub",
   "language": "python",
   "name": "lendingclub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
