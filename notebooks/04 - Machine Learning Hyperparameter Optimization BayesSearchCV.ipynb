{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Training and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is dependent on the execution of `scripts/03 - data_preprocessing.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Up\")\n",
    "import pandas as pd # Data manipulation\n",
    "import sqlite3 # Database connection\n",
    "import numpy as np # Numerical computation\n",
    "import datetime # Date manipulation\n",
    "import seaborn as sns # Plotting\n",
    "import matplotlib.pyplot as plt # Plotting\n",
    "import os # File manipulation\n",
    "import yaml # Config file parsing\n",
    "import argparse # Command line argument parsing\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# ML imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC as SupportVectorClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical, Real\n",
    "\n",
    "print(\"Defining Classes\")\n",
    "# Defining a class for logging messages to a file\n",
    "class Logger:\n",
    "    \"\"\"\n",
    "    A class for logging messages to a file.\n",
    "\n",
    "    Attributes:\n",
    "        config (dict): The configuration settings.\n",
    "        log_dir (str): The directory for storing log files.\n",
    "        tag (str): The tag for identifying the log files.\n",
    "        file_path (str): The path to the log file.\n",
    "        verbose (bool): Flag indicating whether to print log messages to the console.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        # Constructor to initialize the Logger instance\n",
    "        self.config = config  # Store the provided configuration\n",
    "        self.log_dir = config['logging']['out_dir']  # Directory to output logs\n",
    "        self.tag = config['base']['tag']  # Tag for the log (e.g., identifying the run)\n",
    "        # Construct the file path for the log file\n",
    "        self.file_path = os.path.join('outputs', self.tag, self.log_dir, 'log.txt')\n",
    "        self.verbose = config['logging']['verbose']  # Verbose flag to control output\n",
    "        \n",
    "    def log(self, message):\n",
    "        \"\"\"\n",
    "        Logs a message to the log file.\n",
    "\n",
    "        Args:\n",
    "            message (str): The message to be logged.\n",
    "        \"\"\"\n",
    "        # Method to log a message\n",
    "        current_datetime = datetime.datetime.now()  # Get the current date and time\n",
    "        # Format the datetime as a string\n",
    "        datetime_string = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        # Format the log message with a timestamp\n",
    "        log_message = f\"{datetime_string}: {message}\"\n",
    "        if self.verbose:\n",
    "            # If verbose mode is on, print the log message to the console\n",
    "            print(log_message)\n",
    "        with open(self.file_path, \"a\") as f:\n",
    "            # Open the log file in append mode and write the log message\n",
    "            f.write(f'{log_message}\\n')\n",
    "            \n",
    "print(\"Defining Functions\")\n",
    "def bestbandwidth(data):\n",
    "    \"\"\"\n",
    "    Calculate the optimal bandwidth for kernel density estimation using the Silverman's rule of thumb.\n",
    "\n",
    "    Parameters:\n",
    "    data (array-like): The input data for which the bandwidth needs to be calculated.\n",
    "\n",
    "    Returns:\n",
    "    float: The optimal bandwidth value.\n",
    "\n",
    "    \"\"\"\n",
    "    return 1.06*np.std(data)*len(data)**(-1/5)\n",
    "\n",
    "def transform_type(sqlite_type):\n",
    "    \"\"\"\n",
    "    Transforms the SQLite data type to a Python data type.\n",
    "\n",
    "    Parameters:\n",
    "    sqlite_type (str): The SQLite data type.\n",
    "\n",
    "    Returns:\n",
    "    str: The corresponding Python data type.\n",
    "    \"\"\"\n",
    "    if sqlite_type == 'INTEGER':\n",
    "        return 'int'\n",
    "    if sqlite_type == 'REAL':\n",
    "        return 'float'\n",
    "    else:\n",
    "        return 'object'\n",
    "\n",
    "def map_dtype_to_sqlite(col_type):\n",
    "    \"\"\"\n",
    "    Maps a column data type to the corresponding SQLite data type.\n",
    "\n",
    "    Parameters:\n",
    "        col_type (str): The column data type.\n",
    "\n",
    "    Returns:\n",
    "        str: The corresponding SQLite data type.\n",
    "\n",
    "    \"\"\"\n",
    "    if col_type.startswith('int') or col_type == 'bool':\n",
    "        return 'INTEGER'\n",
    "    elif col_type.startswith('float'):\n",
    "        return 'REAL'\n",
    "    else:  # Default case, particularly for 'object' and other unhandled types\n",
    "        return 'TEXT'\n",
    "\n",
    "def time_formatting_function(time_delta):\n",
    "    days = time_delta.days\n",
    "    hours, remainder = divmod(time_delta.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "    formatted_time = \"\"\n",
    "    if days > 0:\n",
    "        formatted_time += f\"{days} days, \"\n",
    "    if hours > 0 or days > 0:  # Include hours if there are any days\n",
    "        formatted_time += f\"{hours} hours, \"\n",
    "    if minutes > 0 or hours > 0 or days > 0:  # Include minutes if there are any hours or days\n",
    "        formatted_time += f\"{minutes} minutes, \"\n",
    "    formatted_time += f\"{seconds} seconds\"\n",
    "\n",
    "    return formatted_time\n",
    "\n",
    "config_file_path = '../config/config.yml'\n",
    "\n",
    "print(f\"Reading Config File {config_file_path}\")\n",
    "\n",
    "# Open and read the configuration file using YAML\n",
    "with open(config_file_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Defining Variables and Creating Directories\")\n",
    "\n",
    "sqlite_file = config['data']['output_sqlite']\n",
    "\n",
    "tag = config['base']['tag']\n",
    "\n",
    "git_repo = config['base']['git_repo']\n",
    "\n",
    "fontsize = config['plotting']['fontsize']\n",
    "figsize_x = config['plotting']['figure_xsize']\n",
    "figsize_y = config['plotting']['figure_ysize']\n",
    "bayes_search_iterations = config['machine_learning']['bayes_search_iterations']\n",
    "random_state = config['machine_learning']['random_state']\n",
    "algorithms = config['machine_learning']['algorithms']\n",
    "n_jobs = config['machine_learning']['n_jobs']\n",
    "\n",
    "optimization_sample_size = config['machine_learning']['optimization_sample_size']\n",
    "\n",
    "out_dir_figures = f\"../outputs/{tag}/figures\"\n",
    "out_dir_log = f\"../outputs/{tag}/log\"\n",
    "out_dir_stats = f\"../outputs/{tag}/stats\"\n",
    "\n",
    "sqlite_file = f\"../outputs/{tag}/data/{sqlite_file}\"\n",
    "\n",
    "columns_of_interest = config['base']['columns_of_interest']\n",
    "\n",
    "print(\"Done with Imports and Definitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Full and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Data\")\n",
    "# Defining the connection to the database\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "# Loading data into dataframe\n",
    "data_fetch_query = f\"\"\"SELECT * \n",
    "                       FROM loans_data_ML\"\"\"\n",
    "\n",
    "loans_data = pd.read_sql_query(data_fetch_query, conn, index_col='id')\n",
    "\n",
    "# Loading data into dataframe\n",
    "data_fetch_query = f\"\"\"SELECT id, total_pymnt\n",
    "                       FROM loans_data\"\"\"\n",
    "\n",
    "loans_data_paymnts = pd.read_sql_query(data_fetch_query, conn)\n",
    "\n",
    "loans_data_paymnts = loans_data_paymnts[loans_data_paymnts['id'].apply(lambda x: x in loans_data.index)]\n",
    "\n",
    "combined_data = pd.merge(loans_data, loans_data_paymnts, on='id', how='inner')\n",
    "combined_data['Profit_or_Loss'] = combined_data['total_pymnt'] - combined_data['loan_amnt']\n",
    "\n",
    "# Closing connection\n",
    "conn.close()\n",
    "\n",
    "print(\"Done Loading Full and Sample Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating sample data for ML hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = combined_data.sample(optimization_sample_size, random_state=random_state)\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # Since we're setting this at the sys level, it should not be overridden\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # Also affect subprocesses\n",
    "   \n",
    "X_opt, y_opt = sample_data.drop('loan_status', axis='columns'), sample_data['loan_status']\n",
    "\n",
    "X, y = combined_data.drop('loan_status', axis='columns'), combined_data['loan_status']\n",
    "\n",
    "ML_columns = loans_data.drop('loan_status', axis='columns').columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "X_train, X_train_profit_or_loss = X_train.drop(['id', 'total_pymnt', 'Profit_or_Loss'], axis='columns'), X_train['Profit_or_Loss']\n",
    "\n",
    "X_opt, X_opt_profit_or_loss = X_opt.drop(['id', 'total_pymnt', 'Profit_or_Loss'], axis='columns'), X_opt['Profit_or_Loss']\n",
    "\n",
    "cummulative_results = None\n",
    "\n",
    "average_profit_0 = np.mean(combined_data[combined_data['loan_status'] == 0]['Profit_or_Loss'])\n",
    "average_profit_1 = np.mean(combined_data[combined_data['loan_status'] == 1]['Profit_or_Loss'])\n",
    "\n",
    "print(\"Done with Separating sample data for ML hyperparameter optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Profit Scorer and Callback Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit_scorer(y, y_pred_proba):\n",
    "    # Compute probabilities for class at position [1]\n",
    "    probabilities = y_pred_proba\n",
    "    \n",
    "    # Assuming X_profit_or_loss is globally accessible and represents the profit or loss for each instance\n",
    "    global average_profit_0, average_profit_1\n",
    "\n",
    "    profit_or_loss_array = average_profit_0*(1-y) + average_profit_1*y\n",
    "    # Initialize profits array\n",
    "    profits = np.zeros(50)\n",
    "    for i, thresh in enumerate(np.linspace(0, 1, 50)):\n",
    "        # Calculate profit for each threshold\n",
    "        profits[i] = profit_or_loss_array[probabilities <= thresh].sum()\n",
    "    # Calculate and return max relative profit\n",
    "    max_relative_profit = np.max(profits) / np.sum(profit_or_loss_array)\n",
    "    return max_relative_profit\n",
    "\n",
    "def callback(res):\n",
    "    global time_start\n",
    "    global logger\n",
    "    global bayes_search_iterations\n",
    "    global algorithm\n",
    "\n",
    "    time_elapsed = datetime.datetime.now() - time_start\n",
    "    eta = time_elapsed / len(res.x_iters) * bayes_search_iterations - time_elapsed\n",
    "\n",
    "    logger.log(f\"Iteration {len(res.x_iters)} / {bayes_search_iterations} for {algorithm}\")\n",
    "    logger.log(f\"Time Elapsed: {time_formatting_function(time_elapsed)}\")\n",
    "    logger.log(f\"ETA: {time_formatting_function(eta)}\")\n",
    "    logger.log(f\"Best Score so far: {np.max(-res.func_vals)}\")\n",
    "    logger.log(f\"Best Parameters so far: {res.x_iters[np.argmax(-res.func_vals)]}\")\n",
    "    logger.log(f\"Current Parameters: {res.x}\")\n",
    "    logger.log(f'Average score: {np.mean(-res.func_vals)}')\n",
    "\n",
    "max_profit_scorer = make_scorer(profit_scorer, greater_is_better=True, needs_proba=True)\n",
    "\n",
    "print(\"Done with Defining Profit Scorer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': Real(1e-10, 1e10, 'log-uniform'),\n",
    "    'solver': ['lbfgs'],\n",
    "    'penalty': ['l2'],\n",
    "    'class_weight': ['balanced'],\n",
    "    'max_iter': [1000000]\n",
    "    }\n",
    "for value in param_grid.values():\n",
    "    type_str = str(type(value))\n",
    "    if type_str != \"<class 'list'>\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\"LogisticRegression\":{\n",
    "                    'C': Real(1e-10, 1e10, 'log-uniform'),\n",
    "                    'solver': ['lbfgs'],\n",
    "                    'penalty': ['l2'],\n",
    "                    'class_weight': ['balanced'],\n",
    "                    'max_iter': [1000000],\n",
    "                    'random_state': [random_state]\n",
    "                    },\n",
    "              \"RandomForestClassifier\":{\n",
    "                    'n_estimators': Integer(10, 1000, 'log-uniform'),\n",
    "                    'criterion': Categorical(['gini', 'entropy']),\n",
    "                    'max_features': Categorical(['sqrt', 'log2']),\n",
    "                    'max_depth': Integer(1, 100),\n",
    "                    'min_samples_split': Integer(2, 50),\n",
    "                    'min_samples_leaf': Integer(1, 10),\n",
    "                    'bootstrap': Categorical([True, False]),\n",
    "                    'class_weight': ['balanced'],\n",
    "                    'random_state': [random_state]\n",
    "                    },\n",
    "               \"SupportVectorClassifier\":{\n",
    "                    'C': Real(1e-6, 1e+6, 'log-uniform'),\n",
    "                    'kernel': Categorical(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "                    'degree': Integer(1, 5),  # only used with 'poly' kernel\n",
    "                    'gamma': Categorical(['scale', 'auto']),\n",
    "                    'class_weight': ['balanced'],\n",
    "                    'probability': [True],\n",
    "                    'random_state': [random_state]\n",
    "               }}\n",
    "\n",
    "def is_numerical_param(param):\n",
    "    return (str(type(param)) != \"<class 'list'>\") and (str(type(param)) != \"<class 'skopt.space.space.Categorical'>\")\n",
    "\n",
    "initial_params = {algorithm:{key: value[0] for key, value in param_grids[algorithm].items() if str(type(value)) == \"<class 'list'>\"} for algorithm in param_grids.keys()}\n",
    "n_num_params_dict = {algorithm:0 for algorithm in algorithms}\n",
    "for algorithm in algorithms:\n",
    "    param_grid = param_grids[algorithm]\n",
    "    for value in param_grid.values():\n",
    "        if is_numerical_param(value):\n",
    "            n_num_params_dict[algorithm] += 1\n",
    "\n",
    "n_num_params = np.max([i for i in n_num_params_dict.values()])\n",
    "\n",
    "fig, ax = plt.subplots(n_num_params, len(algorithms), figsize = [len(algorithms)*figsize_x, n_num_params*figsize_y])\n",
    "\n",
    "\n",
    "for i, algorithm in enumerate(algorithms):\n",
    "    print(f\"Optimizing {algorithm} Hyperparameters\")\n",
    "    param_grid = param_grids[algorithm]\n",
    "    ini_params = initial_params[algorithm]\n",
    "    if algorithm == \"LogisticRegression\":\n",
    "        clf = LogisticRegression(**ini_params)\n",
    "    elif algorithm == \"RandomForestClassifier\":\n",
    "        clf = RandomForestClassifier(**ini_params) \n",
    "    elif algorithm == \"SupportVectorClassifier\":\n",
    "        clf = SupportVectorClassifier(**ini_params)\n",
    "\n",
    "\n",
    "    grid_search = BayesSearchCV(estimator=clf,\n",
    "                                    search_spaces=param_grid,\n",
    "                                    cv=5,\n",
    "                                    verbose=False,\n",
    "                                    scoring=max_profit_scorer,\n",
    "                                    n_iter=bayes_search_iterations,\n",
    "                                    n_jobs=n_jobs,\n",
    "                                    random_state=random_state)\n",
    "    \n",
    "    time_start = datetime.datetime.now()\n",
    "\n",
    "    np.int = int \n",
    "    grid_search.fit(X_opt, y_opt, callback=callback)\n",
    "\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    results['ML_model'] = [algorithm]*len(results)\n",
    "\n",
    "    if cummulative_results is None:\n",
    "        cummulative_results = results.copy()\n",
    "    else:\n",
    "        cummulative_results = pd.concat([cummulative_results, results.copy()])\n",
    "\n",
    "    print(f\"Plotting Results for {algorithm} Hyperparameter Optimization\")\n",
    "\n",
    "    for j, param in enumerate([key for key, value in param_grid.items() if is_numerical_param(value)]):\n",
    "        x_plot = results['params'].apply(lambda x: x[param])\n",
    "        ax[j, i].hist(x_plot)\n",
    "        ax[j, i].set_yscale('log')\n",
    "        ax[j, i].set_xlabel(param, fontsize=fontsize)\n",
    "        ax[j, i].set_ylabel(f'Count', fontsize=fontsize)\n",
    "        ax[j, i].set_title(f'{algorithm} | {param} Distribution on Bayes Search', fontsize=fontsize)\n",
    "  \n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(out_dir_figures, \"BayesSearchCV_parameter_Distributions.png\"))\n",
    "\n",
    "clear_output(wait=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bw = bestbandwidth(cummulative_results['mean_test_score'].values)\n",
    "maxX = cummulative_results['mean_test_score'].max()\n",
    "minX = cummulative_results['mean_test_score'].min()\n",
    "nBins = int((maxX - minX)/best_bw)\n",
    "bins = np.linspace(minX, maxX, nBins)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[figsize_x, figsize_y])\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    data = cummulative_results[cummulative_results['ML_model'] == algorithm]['mean_test_score']\n",
    "    sns.histplot(data, kde=True, label=algorithm, ax=ax, bins=bins, element='step')\n",
    "    \n",
    "ax.set_xlabel(\"Mean Relative Profit Score\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Count\", fontsize=fontsize)\n",
    "ax.set_title(\"Distribution of Test Scores in Bayesian Search For Each Machine Learning Algorithm\", fontsize=fontsize)\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Integer, Categorical, Real\n",
    "\n",
    "a = Real(1e-1,1, 'log-uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LendingClub",
   "language": "python",
   "name": "lendingclub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
